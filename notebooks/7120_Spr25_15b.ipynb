{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Continuing our discussion of DTs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## DTs for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "X = california.data\n",
    "y = california.target\n",
    "\n",
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "`X` and `y` are both numpy arrays. Let's build a dataframe to more easily assess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "df['Price'] = california.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(california.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Split, Fit, Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train a decision tree regressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = tree_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {np.sqrt(mse):.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Plot the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(tree_reg, filled=True, feature_names=california.feature_names, rounded=True)\n",
    "plt.title('Regression Tree for California Housing Prices')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print information about the tree\n",
    "print(f\"Number of nodes: {tree_reg.tree_.node_count}\")\n",
    "print(f\"Tree depth: {tree_reg.get_depth()}\")\n",
    "print(f\"Number of leaves: {tree_reg.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = tree_reg.feature_importances_\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(california.feature_names, importance)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance for Housing Price Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "most_important_feature = np.argmax(importance)\n",
    "print(f\"Most important feature: {california.feature_names[most_important_feature]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Plot Predictions Based Only on Most Important Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified model using only the most important feature\n",
    "X_single = X_train[:, [most_important_feature]]\n",
    "single_tree = DecisionTreeRegressor(max_depth=3)\n",
    "single_tree.fit(X_single, y_train)\n",
    "\n",
    "# Create a range of values for plotting\n",
    "X_range = np.linspace(min(X[:, most_important_feature]), \n",
    "                       max(X[:, most_important_feature]), \n",
    "                       1000).reshape(-1, 1)\n",
    "y_range = single_tree.predict(X_range)\n",
    "\n",
    "# Plot with a proper step function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[:, most_important_feature], y_train, alpha=0.3, label='Training data')\n",
    "plt.scatter(X_test[:, most_important_feature], y_test, alpha=0.3, label='Test data')\n",
    "plt.step(X_range.flatten(), y_range, 'r-', where='post', linewidth=2, label='Tree predictions')\n",
    "\n",
    "plt.xlabel(f'Feature: {california.feature_names[most_important_feature]}')\n",
    "plt.ylabel('House Price (100k$)')\n",
    "plt.title('Regression Tree Predictions vs Most Important Feature')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. **Clear Decision Thresholds**: The vertical lines in the red prediction line occur at specific MedInc values where the decision tree makes splits (approximately at 1.0, 1.8, 2.6, 3.4, 4.3, 5.4, 6.8, and so on).\n",
    "\n",
    "2. **Piecewise Constant Predictions**: Between any two thresholds, the tree makes exactly the same prediction for all houses regardless of small variations in income within that range.\n",
    "\n",
    "3. **Increasing Steps**: Each \"step up\" represents a new leaf node with a higher predicted house price. The decision tree has correctly captured the overall positive relationship between median income and house prices.\n",
    "\n",
    "4. **Simplified Approximation**: This stepped pattern shows how regression trees approximate continuous relationships with a series of flat regions - making them less flexible than something like linear regression for truly linear relationships, but more flexible for capturing non-linear patterns.\n",
    "\n",
    "5. **Limited Resolution**: With max_depth=3, the tree can only make a limited number of splits, which is why you see relatively few steps rather than a more granular approximation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Post-Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a model\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    return {\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Implementation is non-trivial in SKL. Here is the key bit:\n",
    "\n",
    "- generate a full tree\n",
    "- use `cost_complexity_pruning_path` function to calculate the exact alpha values where the optimal tree structure changes. Each alpha corresponds to a specific pruning decision.\n",
    "- fit and evaluate one DT for each alpha value of interest\n",
    "- use the best resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate a sequence of alphas to test\n",
    "# Create a fully grown tree to use for pruning paths\n",
    "full_tree = DecisionTreeRegressor(random_state=42)\n",
    "path = full_tree.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "# Extract the alphas\n",
    "ccp_alphas = path.ccp_alphas\n",
    "ccp_alphas = ccp_alphas[:-1]  # Remove the last alpha which gives a trivial tree\n",
    "\n",
    "# Limit the number of alphas to a reasonable range\n",
    "if len(ccp_alphas) > 10:\n",
    "    # Take a subset of alphas for demonstration\n",
    "    indices = np.linspace(0, len(ccp_alphas) - 1, 10, dtype=int)\n",
    "    ccp_alphas = ccp_alphas[indices]\n",
    "\n",
    "# Create and evaluate models with different ccp_alpha values\n",
    "models = []\n",
    "results = []\n",
    "\n",
    "for alpha in ccp_alphas:\n",
    "    model = DecisionTreeRegressor(ccp_alpha=alpha, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "    \n",
    "    # Store model statistics\n",
    "    result = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "    result['alpha'] = alpha\n",
    "    result['n_nodes'] = model.tree_.node_count\n",
    "    result['depth'] = model.get_depth()\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### Extract Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to arrays for easier plotting\n",
    "alphas = [r['alpha'] for r in results]\n",
    "train_rmse = [r['train_rmse'] for r in results]\n",
    "test_rmse = [r['test_rmse'] for r in results]\n",
    "n_nodes = [r['n_nodes'] for r in results]\n",
    "\n",
    "# Find the best alpha based on test performance\n",
    "best_alpha_idx = np.argmin(test_rmse)\n",
    "best_alpha = alphas[best_alpha_idx]\n",
    "print(f\"Best alpha: {best_alpha:.6f}\")\n",
    "print(f\"Number of nodes: {n_nodes[best_alpha_idx]}\")\n",
    "print(f\"Train RMSE: {train_rmse[best_alpha_idx]:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse[best_alpha_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Compare with our simple tree, with max depth 3, 15 nodes, 8 leaves → Test RMSE of 0.80!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot RMSE\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Alpha')\n",
    "ax1.set_ylabel('RMSE', color=color)\n",
    "ax1.plot(alphas, train_rmse, 'o-', color=color, label='Train RMSE')\n",
    "ax1.plot(alphas, test_rmse, 's-', color='tab:orange', label='Test RMSE')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Create a second y-axis for number of nodes\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('Number of nodes', color=color)\n",
    "ax2.plot(alphas, n_nodes, '^-', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Add legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + ['Number of nodes'], loc='upper left')\n",
    "\n",
    "plt.title('Effect of ccp_alpha on Tree Complexity and Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### Look at Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = DecisionTreeRegressor(ccp_alpha=best_alpha, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Compare with an unpruned tree\n",
    "unpruned_model = DecisionTreeRegressor(random_state=42)\n",
    "unpruned_model.fit(X_train, y_train)\n",
    "unpruned_result = evaluate_model(unpruned_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Unpruned tree - Nodes: {unpruned_model.tree_.node_count}, Depth: {unpruned_model.get_depth()}\")\n",
    "print(f\"Pruned tree - Nodes: {best_model.tree_.node_count}, Depth: {best_model.get_depth()}\")\n",
    "print(f\"Unpruned tree - Train RMSE: {unpruned_result['train_rmse']:.4f}, Test RMSE: {unpruned_result['test_rmse']:.4f}\")\n",
    "print(f\"Pruned tree - Train RMSE: {train_rmse[best_alpha_idx]:.4f}, Test RMSE: {test_rmse[best_alpha_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Visually Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for both models using the most important feature\n",
    "feature_importances = best_model.feature_importances_\n",
    "most_important_feature = np.argmax(feature_importances)\n",
    "print(f\"\\nMost important feature: {california.feature_names[most_important_feature]}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_test[:, most_important_feature], y_test, alpha=0.3, label='Test data')\n",
    "\n",
    "# Create a range of values for the most important feature\n",
    "X_range = np.linspace(min(X[:, most_important_feature]), \n",
    "                       max(X[:, most_important_feature]), \n",
    "                       1000).reshape(-1, 1)\n",
    "X_dummy = np.tile(np.median(X_test, axis=0), (X_range.shape[0], 1))\n",
    "X_dummy[:, most_important_feature] = X_range[:, 0]\n",
    "\n",
    "# Generate predictions for both models\n",
    "y_unpruned = unpruned_model.predict(X_dummy)\n",
    "y_pruned = best_model.predict(X_dummy)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.plot(X_range, y_unpruned, 'r-', alpha=0.7, label='Unpruned tree predictions')\n",
    "plt.plot(X_range, y_pruned, 'g-', linewidth=2, label='Pruned tree predictions')\n",
    "\n",
    "plt.xlabel(f'Feature: {california.feature_names[most_important_feature]}')\n",
    "plt.ylabel('House Price (100k$)')\n",
    "plt.title('Comparison of Pruned vs Unpruned Tree Predictions')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "This result shows a clear demonstration of cost-complexity pruning and its effect on decision tree performance.\n",
    "\n",
    "Top Graph (Alpha vs. RMSE and Tree Size):\n",
    "\n",
    "1. **Tree Complexity (Green Line)**: As alpha increases, the number of nodes dramatically decreases from over 25,000 to nearly 0, showing how pruning effectively reduces tree size.\n",
    "\n",
    "2. **Training RMSE (Blue Line)**: Starting near zero with the unpruned tree (showing overfitting), it gradually increases as the tree is pruned, indicating reduced ability to fit the training data perfectly.\n",
    "\n",
    "3. **Test RMSE (Orange Line)**: This is the critical curve. It shows a slight but meaningful improvement (dip) around alpha = 0.000043, before worsening at higher alpha values. This indicates the sweet spot where pruning improves generalization.\n",
    "\n",
    "Numerical Results:\n",
    "\n",
    "- The best alpha (0.000043) reduced the tree from 27,665 nodes to 3,647 nodes (about 87% reduction)\n",
    "- Depth decreased from 34 to 23\n",
    "- Training RMSE increased from 0.0000 (perfect fit) to 0.2309 (expected with pruning)\n",
    "- **Test RMSE improved from 0.7266 to 0.6967** - this is the key result showing improved generalization\n",
    "\n",
    "Bottom Graph (Predictions Comparison):\n",
    "\n",
    "- **Unpruned Tree (Red Line)**: Shows highly erratic, jagged predictions that attempt to fit every data point\n",
    "- **Pruned Tree (Green Line)**: Shows smoother, more stable predictions that better capture the general trend\n",
    "\n",
    "This demonstrates the classic bias-variance tradeoff in machine learning. The unpruned tree has low bias (fits training data perfectly) but high variance (generalizes poorly). The pruned tree accepts some bias (doesn't fit training data perfectly) but achieves lower variance, resulting in better overall performance on new data.\n",
    "\n",
    "The optimal pruning level maintains the important structural aspects of the tree while eliminating branches that mainly fit noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Pre-Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Find the set of hyperparameters that result in the best performing tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Fit Models using `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Define the hyperparameter space and perform exhaustive search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8]\n",
    "}\n",
    "\n",
    "# Create a base decision tree regressor\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    tree_reg,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "print(\"Starting grid search...\")\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Grid search completed!\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "**Note:** use `n_jobs=-1` to make use of all cpu cores. Without this setting, the search took 38.76 seconds on my 5yr old intel macbook. With it, the time was reduced to 7.6 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "#### Extract Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_  # Convert back to RMSE from negative RMSE\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best CV RMSE: {best_score:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "\n",
    "# Compare with default model\n",
    "default_model = DecisionTreeRegressor(random_state=42)\n",
    "default_model.fit(X_train, y_train)\n",
    "default_pred = default_model.predict(X_test)\n",
    "default_rmse = np.sqrt(mean_squared_error(y_test, default_pred))\n",
    "default_r2 = r2_score(y_test, default_pred)\n",
    "\n",
    "print(\"\\nComparison with default model:\")\n",
    "print(f\"Default model - Test RMSE: {default_rmse:.4f}, R²: {default_r2:.4f}\")\n",
    "print(f\"Tuned model - Test RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
    "print(f\"Improvement: {((default_rmse - test_rmse) / default_rmse * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Reminder:\n",
    "- CV RMSE is used to find the best parameter set, using CV on the training data\n",
    "- Test RMSE is the estimated prediction error for the resulting model, based on the test data\n",
    "\n",
    "The process is:\n",
    "\n",
    "1. Split data into training and test sets\n",
    "2. Use cross-validation within the training set to find the best hyperparameters (CV RMSE)\n",
    "3. *Train the final model with the best hyperparameters on the entire training set*\n",
    "4. Evaluate this model on the test set (Test RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### Make Fancy Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results of grid search for max_depth\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Extract results for different max_depth values (averaging over other parameters)\n",
    "results = grid_search.cv_results_\n",
    "max_depths = param_grid['max_depth']\n",
    "mean_test_scores = {}\n",
    "mean_train_scores = {}\n",
    "\n",
    "for depth in max_depths:\n",
    "    # Get indices where max_depth equals current depth\n",
    "    indices = [i for i, params in enumerate(results['params']) \n",
    "               if params['max_depth'] == depth]\n",
    "    \n",
    "    # Calculate average scores for this depth\n",
    "    mean_test_scores[depth] = -np.mean([results['mean_test_score'][i] for i in indices])\n",
    "    mean_train_scores[depth] = -np.mean([results['mean_train_score'][i] for i in indices])\n",
    "\n",
    "# Convert None to a plottable value (fixing the TypeError)\n",
    "# First find the maximum numeric depth\n",
    "numeric_depths = [d for d in max_depths if d is not None]\n",
    "max_numeric_depth = max(numeric_depths) if numeric_depths else 0\n",
    "# Then use this to replace None with a value that's slightly larger\n",
    "plot_depths = [d if d is not None else max_numeric_depth + 5 for d in max_depths]\n",
    "\n",
    "test_scores = [mean_test_scores[d] for d in max_depths]\n",
    "train_scores = [mean_train_scores[d] for d in max_depths]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(plot_depths, train_scores, 'o-', label='Training RMSE')\n",
    "plt.plot(plot_depths, test_scores, 's-', label='Cross-validation RMSE')\n",
    "\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Effect of max_depth on Model Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark the best depth\n",
    "best_depth = best_params['max_depth']\n",
    "best_depth_value = best_depth if best_depth is not None else max_numeric_depth + 5\n",
    "plt.axvline(x=best_depth_value, color='r', linestyle='--', alpha=0.5)\n",
    "plt.text(best_depth_value, min(test_scores), f'Best: {best_depth}', \n",
    "         rotation=90, verticalalignment='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "This graph shows how the max_depth parameter affects model performance for decision trees on the California Housing dataset:\n",
    "\n",
    "1. **Training vs. Cross-validation Performance**: The blue line (Training RMSE) decreases continuously as max_depth increases, while the orange line (Cross-validation RMSE) initially decreases but then reaches a minimum and begins to increase slightly.\n",
    "\n",
    "2. **Optimal Depth**: The vertical dashed line marks the best max_depth value of 15, which was identified by the grid search. This is where cross-validation RMSE is minimized.\n",
    "\n",
    "3. **Overfitting Pattern**: This is a classic illustration of the bias-variance tradeoff:\n",
    "   - At low depths (left side), both training and validation errors are high (underfitting)\n",
    "   - As depth increases, both errors initially decrease\n",
    "   - Beyond depth 10, training error continues to decrease substantially while validation error flattens and slightly increases (beginning of overfitting)\n",
    "   - At depth 25, there's a large gap between training and validation performance (clear overfitting)\n",
    "\n",
    "4. **Diminishing Returns**: The validation curve flattens considerably after depth 10, suggesting minimal improvement from adding additional complexity beyond this point.\n",
    "\n",
    "The graph effectively supports the grid search results, showing that a max_depth of 15 provides good performance on unseen data while avoiding the severe overfitting that occurs with unlimited depth trees. This visualization helps explain why the tuned model achieved a 14.17% improvement over the default model.\n",
    "\n",
    "But `max_depth` of 10 appears to be the best choice - minimal CV RMSE. What gives?\n",
    "\n",
    "*The graph shows the average performance across all combinations of other parameters (min_samples_split and min_samples_leaf) for each max_depth value. However, the grid search selects the best specific combination of all parameters.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "#### `RandomSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Grid search tests all possible combination of parameters. Can take a long time, but results in global optimum (within the specified space).\n",
    "\n",
    "Random approaches can search a wider space in less time, but do not guarantee a global optimum.\n",
    "\n",
    "Instead of arrays of parameter values, `RandomSearchCV` uses values drawn from distributions (can be continuous if appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# specify parameters as distributions!!\n",
    "param_dist = {\n",
    "    'max_depth': randint(3, 30),  # Random integers between 3 and 30\n",
    "    'min_samples_split': randint(2, 30),\n",
    "    'min_samples_leaf': randint(1, 15)\n",
    "}\n",
    "\n",
    "# compare with prior grid\n",
    "# 'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
    "# 'min_samples_split': [2, 5, 10, 20],\n",
    "# 'min_samples_leaf': [1, 2, 4, 8]\n",
    "\n",
    "# Create base decision tree regressor\n",
    "tree_random = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "# Use 100 iterations to sample a good portion of the parameter space\n",
    "random_search = RandomizedSearchCV(\n",
    "    tree_random,\n",
    "    param_dist,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    return_train_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform random search with timing\n",
    "print(\"Starting random search...\")\n",
    "random_start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "random_end_time = time.time()\n",
    "random_elapsed_time = random_end_time - random_start_time\n",
    "print(\"Random search completed!\")\n",
    "print(f\"Random search time: {random_elapsed_time:.2f} seconds ({random_elapsed_time/60:.2f} minutes)\")\n",
    "\n",
    "# Get best parameters and score\n",
    "random_best_params = random_search.best_params_\n",
    "random_best_score = -random_search.best_score_  # Convert back to RMSE from negative RMSE\n",
    "random_best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"\\nBest parameters from random search:\")\n",
    "print(f\"Random search parameters: {random_best_params}\")\n",
    "print(f\"Random search CV RMSE: {random_best_score:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "random_pred = random_best_model.predict(X_test)\n",
    "random_test_rmse = np.sqrt(mean_squared_error(y_test, random_pred))\n",
    "random_test_r2 = r2_score(y_test, random_pred)\n",
    "\n",
    "print(\"\\nTest performance:\")\n",
    "print(f\"Random search - RMSE: {random_test_rmse:.4f}, R²: {random_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Here the time difference is not significant, but it can be a huge benefit.\n",
    "\n",
    "Moreover, we tested a wider space in less time and got better results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "#### YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Here I just let Claude rip and ran what it came up with. Kept it because the visualizations are interesting and the implications are powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "X = california.data\n",
    "y = california.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the parameter grids\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8]\n",
    "}\n",
    "\n",
    "# For random search, we use distributions rather than fixed values\n",
    "param_dist = {\n",
    "    'max_depth': randint(3, 30),  # Random integers between 3 and 30\n",
    "    'min_samples_split': randint(2, 30),\n",
    "    'min_samples_leaf': randint(1, 15)\n",
    "}\n",
    "\n",
    "# Create base decision tree regressors\n",
    "tree_grid = DecisionTreeRegressor(random_state=42)\n",
    "tree_random = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    tree_grid,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "# We'll use 100 iterations to sample a good portion of the parameter space\n",
    "random_search = RandomizedSearchCV(\n",
    "    tree_random,\n",
    "    param_dist,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    return_train_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform grid search with timing\n",
    "print(\"Starting grid search...\")\n",
    "grid_start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_end_time = time.time()\n",
    "grid_elapsed_time = grid_end_time - grid_start_time\n",
    "print(\"Grid search completed!\")\n",
    "print(f\"Grid search time: {grid_elapsed_time:.2f} seconds ({grid_elapsed_time/60:.2f} minutes)\")\n",
    "\n",
    "# Perform random search with timing\n",
    "print(\"\\nStarting random search...\")\n",
    "random_start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "random_end_time = time.time()\n",
    "random_elapsed_time = random_end_time - random_start_time\n",
    "print(\"Random search completed!\")\n",
    "print(f\"Random search time: {random_elapsed_time:.2f} seconds ({random_elapsed_time/60:.2f} minutes)\")\n",
    "\n",
    "# Compare best parameters and scores\n",
    "grid_best_params = grid_search.best_params_\n",
    "grid_best_score = -grid_search.best_score_  # Convert back to RMSE from negative RMSE\n",
    "grid_best_model = grid_search.best_estimator_\n",
    "\n",
    "random_best_params = random_search.best_params_\n",
    "random_best_score = -random_search.best_score_  # Convert back to RMSE from negative RMSE\n",
    "random_best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"\\nBest parameters comparison:\")\n",
    "print(f\"Grid search: {grid_best_params}\")\n",
    "print(f\"Random search: {random_best_params}\")\n",
    "\n",
    "print(\"\\nCV score comparison (RMSE):\")\n",
    "print(f\"Grid search: {grid_best_score:.4f}\")\n",
    "print(f\"Random search: {random_best_score:.4f}\")\n",
    "\n",
    "# Evaluate models on test set\n",
    "grid_pred = grid_best_model.predict(X_test)\n",
    "grid_test_rmse = np.sqrt(mean_squared_error(y_test, grid_pred))\n",
    "grid_test_r2 = r2_score(y_test, grid_pred)\n",
    "\n",
    "random_pred = random_best_model.predict(X_test)\n",
    "random_test_rmse = np.sqrt(mean_squared_error(y_test, random_pred))\n",
    "random_test_r2 = r2_score(y_test, random_pred)\n",
    "\n",
    "print(\"\\nTest performance comparison:\")\n",
    "print(f\"Grid search - RMSE: {grid_test_rmse:.4f}, R²: {grid_test_r2:.4f}\")\n",
    "print(f\"Random search - RMSE: {random_test_rmse:.4f}, R²: {random_test_r2:.4f}\")\n",
    "\n",
    "# Compare efficiency\n",
    "total_grid_combinations = len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])\n",
    "print(\"\\nEfficiency comparison:\")\n",
    "print(f\"Grid search: {total_grid_combinations} combinations evaluated in {grid_elapsed_time:.2f} seconds\")\n",
    "print(f\"Random search: 100 combinations evaluated in {random_elapsed_time:.2f} seconds\")\n",
    "print(f\"Time per combination - Grid: {grid_elapsed_time/total_grid_combinations:.2f}s, Random: {random_elapsed_time/100:.2f}s\")\n",
    "print(f\"Speed improvement: {(grid_elapsed_time/random_elapsed_time):.2f}x\")\n",
    "\n",
    "# Create visualization to compare performance across iterations\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create a subplot for the learning curves\n",
    "plt.subplot(2, 1, 1)\n",
    "grid_results = pd.DataFrame(grid_search.cv_results_)\n",
    "random_results = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "# Sort both results by mean test score\n",
    "grid_sorted = grid_results.sort_values('mean_test_score', ascending=False)\n",
    "random_sorted = random_results.sort_values('mean_test_score', ascending=False)\n",
    "\n",
    "# Plot learning curves - top N results\n",
    "top_n = min(50, len(grid_sorted), len(random_sorted))\n",
    "x_grid = np.arange(top_n)\n",
    "x_random = np.arange(top_n)\n",
    "\n",
    "plt.plot(x_grid, -grid_sorted['mean_test_score'].values[:top_n], 'b-', label='Grid Search CV Score')\n",
    "plt.plot(x_random, -random_sorted['mean_test_score'].values[:top_n], 'r-', label='Random Search CV Score')\n",
    "\n",
    "plt.xlabel('Rank of Model Configuration')\n",
    "plt.ylabel('RMSE (Cross-Validation)')\n",
    "plt.title('Top Model Configurations Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Create a subplot to show parameter space coverage\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Extract unique parameter values from both searches\n",
    "grid_max_depths = grid_results['param_max_depth'].unique()\n",
    "random_max_depths = random_results['param_max_depth'].unique()\n",
    "all_max_depths = sorted(set([d for d in grid_max_depths if d is not None] + \n",
    "                          [d for d in random_max_depths if d is not None]))\n",
    "\n",
    "# Create scatter plot of evaluated configurations\n",
    "plt.scatter(grid_results['param_max_depth'].fillna(0), \n",
    "           grid_results['param_min_samples_leaf'],\n",
    "           c=-grid_results['mean_test_score'], marker='o', s=50, \n",
    "           cmap='Blues', alpha=0.7, label='Grid Search')\n",
    "\n",
    "plt.scatter(random_results['param_max_depth'].fillna(0), \n",
    "           random_results['param_min_samples_leaf'],\n",
    "           c=-random_results['mean_test_score'], marker='x', s=50, \n",
    "           cmap='Reds', alpha=0.7, label='Random Search')\n",
    "\n",
    "plt.colorbar(label='RMSE')\n",
    "plt.xlabel('max_depth (0 = None)')\n",
    "plt.ylabel('min_samples_leaf')\n",
    "plt.title('Parameter Space Coverage')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Top Graph: Model Configurations Comparison\n",
    "\n",
    "1. **Overall Performance**: The random search (red line) consistently achieved better (lower) RMSE scores than the grid search (blue line) across all configurations.\n",
    "\n",
    "2. **Best Models**: The top-ranked random search model achieved an RMSE around 0.625, while the best grid search model's RMSE was about 0.63 - making random search slightly better.\n",
    "\n",
    "3. **Stability**: The random search line shows a more gradual increase in RMSE as we move to lower-ranked configurations, suggesting more consistent performance across the parameter space.\n",
    "\n",
    "4. **Configuration Range**: The grid search line shows more variability and steeper increases, indicating greater performance differences between the best and worst configurations.\n",
    "\n",
    "Bottom Graph: Parameter Space Coverage\n",
    "\n",
    "1. **Exploration Pattern**: \n",
    "   - Grid search (blue dots) appears at specific, regular intervals, showing the structured nature of grid search\n",
    "   - Random search (red X's) is scattered throughout the parameter space, showing its wider exploration\n",
    "\n",
    "2. **Density**: Random search has explored areas of the parameter space that grid search missed completely, particularly combinations with higher min_samples_leaf values (8-12 range).\n",
    "\n",
    "3. **Best Configurations**: The darker colored points (indicating lower RMSE) for random search appear in several regions, including some parameter combinations that grid search didn't evaluate.\n",
    "\n",
    "4. **Parameter Importance**: Both approaches found good models in the max_depth range of 3-10, but random search discovered that higher min_samples_leaf values (around 10-12) could also produce good results.\n",
    "\n",
    "Key Takeaways:\n",
    "\n",
    "1. Random search found better overall solutions while testing fewer combinations\n",
    "2. Random search explored a broader and more diverse parameter space\n",
    "3. Grid search was limited to its predefined grid points, missing potentially valuable regions\n",
    "4. Random search likely benefited from examining parameter combinations outside the grid search's discrete values\n",
    "\n",
    "This perfectly illustrates why random search can be more efficient - it found better models by exploring the parameter space more effectively, despite evaluating fewer total combinations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
