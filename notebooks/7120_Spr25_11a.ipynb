{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook introduces the Scikit-Learn interface for classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Load the packages and configure environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Using the `Default` dataset, which predicts the likelihood of a person defaulting on credit card payments based on their balance, income, and student status (yes / no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data set directly from the web using pandas\n",
    "url = \"https://raw.githubusercontent.com/olearydj/INSY7120/refs/heads/main/notebooks/data/Default.csv\"\n",
    "default = pd.read_csv(url)\n",
    "\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "First, we'll predict `default` based only on credit card `balance`, using logistic regression.\n",
    "\n",
    "We'll use several components from SKL to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Get `X` and `y`. Note that the method below for `y` is exactly equivalent to using `get_dummies` as we've done before:\n",
    "\n",
    "```python\n",
    "y = pd.get_dummies(default['default'], drop_first=True, dtype=int)\n",
    "```\n",
    "\n",
    "For this simple binary case, the code below creates a boolean array based on the comparison `default == 'Yes'`. In Python `True` is equivalent to `1` and `False` to zero, and `astype(int)` converts them to that representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature and target\n",
    "X = default[['balance']].values  # We need 2D array for sklearn\n",
    "y = (default['default'] == 'Yes').astype(int)  # Convert to binary 0/1\n",
    "\n",
    "# use value_counts to see class counts\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "There are 10,000 observations, 9667 of which are False (96.7%), and 333 are True (3.3%).\n",
    "\n",
    "For this example, we'll use simple train-test split validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Logistic Regression uses gradient descent, which is sensitive to differences in scale between the features. Normalize them by z-score standardization using `StandardScaler`. Here we are taking care during fit to calculate the scaler parameters ($\\mu$ and $\\sigma$) using only the training data. The results are used to transform both the training and the test data. This avoids leaking information about the test data (e.g. scale) into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler using training data AND apply that transformation to the same\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# use the scaler coefficients from training to transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "It is simple to fit the `LogisticRegression` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Finally, make predictions. There are two methods used below. `model.predict` generates labels where `model.predict_proba` generates the probabilities. SKL's binary classifiers use a threshold of 0.5 for label assignment. Observations with a predicted probability of greater than 50% get the positive label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Let's look at the relationship between probabilities, predictions, and the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to show samples with predictions and probabilities\n",
    "\n",
    "# Get a sample of predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': y_pred,\n",
    "    'probability': y_pred_prob\n",
    "})\n",
    "\n",
    "# Sort by probability to show examples around the threshold\n",
    "results_df = results_df.sort_values('probability')\n",
    "\n",
    "# Select informative examples (some below and some above the threshold)\n",
    "threshold_examples = pd.concat([\n",
    "    # Examples just below threshold (predict 0)\n",
    "    results_df[(results_df['probability'] > 0.3) & (results_df['probability'] < 0.5)].head(3),\n",
    "    # Examples just above threshold (predict 1)\n",
    "    results_df[(results_df['probability'] >= 0.5) & (results_df['probability'] < 0.7)].head(3)\n",
    "])\n",
    "\n",
    "# Display with formatting\n",
    "pd.set_option('display.precision', 4)\n",
    "print(\"Prediction Examples (threshold = 0.5):\")\n",
    "print(threshold_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "You should be thinking, \"what if I don't want to use a 50% threshold?\" Good question and we will come back to it. TLDR; SKL doesn't support it directly, but it is an important consideration in model development.\n",
    "\n",
    "Let's summarize the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "You should be thinking, \"what are all those numbers?\" Good question, we'll come back to that. Included now for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Interpreting the Predictions\n",
    "\n",
    "These predictions are summarized above as a **Confusion Matrix**. In the case of binary classification it looks like this:\n",
    "\n",
    "|            |                  |        **Predicted**         |        **Predicted**        |\n",
    "| ---------- | ---------------- | :--------------------------: | :-------------------------: |\n",
    "|            |                  |       **Negative (0)**       |      **Positive (1)**       |\n",
    "| **Actual** | **Negative (0)** | True Negative (TN)<br>(2896) | False Positive (FP)<br>(10) |\n",
    "| **Actual** | **Positive (1)** | False Negative (FN)<br>(70)  | True Positive (TP)<br>(24)  |\n",
    "\n",
    "This table follows the scikit-learn format where:\n",
    "- Rows represent actual values (0=No Default, 1=Default), the training labels\n",
    "- Columns represent predicted values (0=No Default, 1=Default)\n",
    "\n",
    "The four cells correspond to:\n",
    "\n",
    "- **Top-Left (2896)**: True Negatives (TN) - Correctly predicted as \"No Default\"\n",
    "- **Top-Right (10)**: False Positives (FP) - Incorrectly predicted as \"Default\"\n",
    "- **Bottom-Left (70)**: False Negatives (FN) - Incorrectly predicted as \"No Default\"\n",
    "- **Bottom-Right (24)**: True Positives (TP) - Correctly predicted as \"Default\"\n",
    "\n",
    "This provides a complete picture of how the model's predictions align with the actual values, showing where the model succeeds and where it makes errors.\n",
    "\n",
    "How would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### Answers - Hide Me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "The data has many more observations with a negative label (default = no) than positive. The first row of the CM shows 2896 + 10 = 2906 actual negatives and only 70 + 24 = 94 actual positives. That's a total of 3,000 observations, which matches the 30% test split we used.\n",
    "\n",
    "If we think only in terms of the number of correct predictions, which are on the diagonal, this does quite well: (2896 + 24) / 3000 = 0.9733 → 97.3% correct.\n",
    "\n",
    "It is also very good at identifying non-defaults (first row): 2896 / (2896 + 10) = 0.9966 → 99.7% correct.\n",
    "\n",
    "Struggles to identify actual defaults (second row): 24 / (70 + 24) = 0.2553 → 25.5% correct.\n",
    "\n",
    "Two types of errors, each with different frequencies and implications:\n",
    "\n",
    "- False Positives (10) - predicting default when there is none\n",
    "- False Negatives (70) - missing actual defaults\n",
    "\n",
    "In these results, False Negatives are 7x more common than False Positives. In a lending business, failing to identify potential defaults (FNs) is typically more costly than incorrectly flagging bad borrowers (FPs). The high number of FNs suggests the model might not adequately address the business need despite seeming very accurate. Poor discriminator for this scenario.\n",
    "\n",
    "Why might this model be biased towards predicting no default?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Some fancy matplotlib work courtesy of Claude... Note that the actual and predicted dots should both appear on the 1.0 and 0.0 lines. They are shown stacked above and below those for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results with better annotations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Create small vertical offsets for clarity\n",
    "offset_actual = 0.02  # Offset for actual values\n",
    "offset_pred = -0.02   # Offset for predicted values\n",
    "\n",
    "# Plot actual values (blue dots)\n",
    "plt.scatter(X_test, y_test + offset_actual, color='blue', alpha=0.5, label='Actual')\n",
    "\n",
    "# Plot predicted values (red dots)\n",
    "plt.scatter(X_test, y_pred + offset_pred, color='red', alpha=0.5, label='Predicted')\n",
    "\n",
    "# Plot the decision boundary\n",
    "balance_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 100).reshape(-1, 1)\n",
    "balance_range_scaled = scaler.transform(balance_range)\n",
    "y_prob = model.predict_proba(balance_range_scaled)[:, 1]\n",
    "plt.plot(balance_range, y_prob, 'g-', label='Probability of Default')\n",
    "plt.axhline(y=0.5, color='k', linestyle='--', label='Decision Boundary')\n",
    "\n",
    "# Create annotation boxes for False Negatives and False Positives\n",
    "# False Negatives (top region)\n",
    "fn_rect = patches.Rectangle((1000, 0.99), 930, 0.06, linewidth=2, edgecolor='r', facecolor='none')\n",
    "plt.gca().add_patch(fn_rect)\n",
    "plt.text(1500, 1.08, \"False Negatives\", color='red', fontsize=16, ha='center')\n",
    "\n",
    "# False Positives (bottom right region)\n",
    "fp_rect = patches.Rectangle((1955, -0.01), 600, 0.06, linewidth=2, edgecolor='r', facecolor='none')\n",
    "plt.gca().add_patch(fp_rect)\n",
    "plt.text(2250, -0.07, \"False Positives\", color='red', fontsize=16, ha='center')\n",
    "\n",
    "# Adjust y-axis to accommodate offsets and annotations\n",
    "plt.ylim(-0.15, 1.15)\n",
    "\n",
    "plt.xlabel('Credit Card Balance')\n",
    "plt.ylabel('Default Probability')\n",
    "plt.title('Logistic Regression: Predicting Default based on Balance')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "We can observe a few things here:\n",
    "\n",
    "- The decision boundary (dashed line) at 0.5 probability corresponds to a balance of approximately \\$2000, where the model transitions from predicting \"No Default\" to \"Default\".\n",
    "- Most accounts with balances below \\$2k have low default probabilities and are classified as non-defaults (red dots at bottom)\n",
    "- Most accounts with balances above \\$2k have high default probabilities and are classified as defaults (red dots at top)\n",
    "- There are several misclassifications evident:\n",
    "  - False negatives (actual = default, prediction = non-default): blue dots at y=1 without matching red dot\n",
    "  - False positives (actual = non-default, prediction = default): blue dots at y=0 without matching red dot\n",
    "\n",
    "Most FNs occur with balances between ≈\\$1k and 2k. FPs are less common and occur at higher balances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Interpret Model Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Extract model coefficients from the fitted model and interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model coefficients\n",
    "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
    "print(f\"Coefficient for balance: {model.coef_[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "These don't match the results in ISL Chapter 4:\n",
    "\n",
    "![](images/isl-tbl-4.1.png)\n",
    "\n",
    "Why? We've standardized the features so they are in the scale of standard deviations of the feature values. Where ISL's result is interpreted in terms of $1 changes to the balance, our results must be interpreted as follows:\n",
    "\n",
    "> For every one *standard deviation* increase in `balance`, the *log-odds* of `default` increase by 2.6678."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "If we want to interpret the coefficients in the original scale, we need to reverse the effects of the `StandardScaler`. Here's the mathematical approach:\n",
    "\n",
    "#### The Standardization Process\n",
    "\n",
    "A feature $X$ is standardized using:\n",
    "\n",
    "$$X_{scaled} = \\frac{X - \\mu_X}{\\sigma_X}$$\n",
    "\n",
    "Where $\\mu_X$ is the mean and $\\sigma_X$ is the standard deviation of $X$.\n",
    "\n",
    "#### Logistic Regression with Standardized Data\n",
    "\n",
    "For a model fit on standardized data:\n",
    "\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{scaled}$$\n",
    "\n",
    "Substituting the standardization equation:\n",
    "\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\frac{X - \\mu_X}{\\sigma_X}$$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = \\left[\\beta_0 - \\beta_1\\frac{\\mu_X}{\\sigma_X}\\right] + \\frac{\\beta_1}{\\sigma_X}X$$\n",
    "\n",
    "#### Coefficients in Original Scale\n",
    "\n",
    "From this rearrangement, we can see:\n",
    "\n",
    "1. The coefficient in original scale: $\\beta_{1,orig} = \\frac{\\beta_1}{\\sigma_X}$\n",
    "\n",
    "2. The intercept in original scale: $\\beta_{0,orig} = \\beta_0 - \\beta_1\\frac{\\mu_X}{\\sigma_X}$\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "After transformation, we can properly interpret:\n",
    "- For each 1-unit increase in the original feature, the log-odds increase by $\\beta_{1,orig}$\n",
    "- When the original feature equals zero, the log-odds equal $\\beta_{0,orig}$\n",
    "\n",
    "This manual transformation ensures we maintain the statistical benefits of standardization during model training while enabling interpretable coefficients in the original units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scaling parameters from the scaler\n",
    "# first index, only one feature\n",
    "std_dev_balance = scaler.scale_[0]\n",
    "mean_balance = scaler.mean_[0]\n",
    "\n",
    "# Convert the coefficient back to the original scale\n",
    "coef_original = model.coef_[0][0] / std_dev_balance\n",
    "\n",
    "# Convert the intercept back to the original scale\n",
    "intercept_original = model.intercept_[0] - (model.coef_[0][0] * mean_balance / std_dev_balance)\n",
    "\n",
    "print(f\"Standard deviation of balance: {std_dev_balance:.4f}\")\n",
    "print(f\"Mean of balance: {mean_balance:.4f}\")\n",
    "print(f\"Coefficient in original scale: {coef_original:.4f}\")\n",
    "print(f\"Intercept in original scale: {intercept_original:.4f}\")\n",
    "print(f\"For every $1 increase in balance, log-odds of default increases by {coef_original:.4f}\")\n",
    "print(f\"The log-odds of default when balance = 0 is {intercept_original:.4f}\")\n",
    "\n",
    "# The equation in the original scale would be:\n",
    "print(\"\\nLogistic regression equation in original scale:\")\n",
    "print(f\"log-odds(default) = {intercept_original:.4f} + {coef_original:.4f} × balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "These results are close to those obtained by ISL (-10.6513 + 0.0055 x balance). The difference in the intercept could come down to some combination of the splitting method, randomization, precision limits, and optimization method used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Also, introducing SKL pipelines.\n",
    "\n",
    "Same problem, all predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['student']\n",
    "numerical_cols = ['balance', 'income']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "SKL provides many workflow tools. The `Pipeline` class brings them all together, giving a way to represent all the steps in the modeling process concisely for repeatable, interpretable results.\n",
    "\n",
    "The Pipeline encapsulates a complete machine learning workflow by sequentially applying a list of transformers and estimators. This provides several key benefits for engineers:\n",
    "\n",
    "1. Code organization - keeps preprocessing, feature engineering, and model training steps in a single object\n",
    "2. Prevention of data leakage - ensures transformations learned on training data are properly applied to test data\n",
    "3. Parameter tuning - enables grid search across all pipeline components simultaneously\n",
    "4. Production deployment - packages the entire workflow into a single estimator object with standard fit/predict methods\n",
    "\n",
    "In this example we will chain preproccessing (scaling and dummy transformations) and classification together. This allows us to call fit() and predict() on the entire sequence as if it were a single estimator.\n",
    "\n",
    "To achieve this we will first use `ColumnTransformer` to define the feature transformations for the numerical (`StandardScaler`) and categorical (`OneHotEncoder`) columns defined above. `OneHotEncoder` is the equivalent of `get_dummies` from pandas or the boolean mask approached we used early in this notebook. We use it here instead of those methods as `OneHotEncoder` is compatable with `Pipeline`.\n",
    "\n",
    "The `ColumnTransformer` specifies a list of tuples, where each tuple includes the name, method, and target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor for mixed data types\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Once the preprocessor is defined we can use it as a **step** in the `Pipeline`. The steps are simply a list of tuples, where each tuple includes the name and process to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with preprocessing and logistic regression\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "With the end-to-end process defined, preparing and splitting the data proceeds as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = default[numerical_cols + categorical_cols]\n",
    "y = (default['default'] == 'Yes').astype(int)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Rather than performing each step (scaling, fitting) separately, we fit the pipeline itself on the training data and make predictions using the test data. As before we use both `predict` to generate labels, and `predict_proba` for the associated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model pipeline\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "y_pred_prob = model_pipeline.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Finally, we output the resulting confusion matrix and classification report scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Note that there is only one change in this result - one prediction changes from FN to TP.\n",
    "\n",
    "Now we extract and interpret the coefficients. The results of each step are stored in the `named_steps` attribute of the pipeline object. For example, the classifier's coefficients can be accessed via the usual `coef_` and `intercept_` attributes of the `classifier` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients from the model\n",
    "coefficients = model_pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Calculate the intercept in original scale\n",
    "intercept = model_pipeline.named_steps['classifier'].intercept_[0]\n",
    "\n",
    "# Get the column names after one-hot encoding\n",
    "encoded_feature_names = numerical_cols + ['student_Yes']\n",
    "\n",
    "# Create a DataFrame to display the coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': encoded_feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(\"\\nModel Coefficients:\")\n",
    "print(coef_df)\n",
    "\n",
    "print(f\"\\nIntercept: {intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Interpreting these coefficients requires caution for several reasons:\n",
    "\n",
    "1. Standardized coefficients: Since numerical predictors are standardized, the coefficients represent changes in log-odds per standard deviation increase, not per unit increase. This makes direct comparison between raw coefficients potentially misleading.\n",
    "2. Log-odds scale: The coefficients represent changes in log-odds, which aren't intuitively interpretable like linear regression coefficients. Converting to odds ratios (by exponentiating) would make them more interpretable.\n",
    "3. Feature correlation: Balance, income, and student status may be correlated, affecting the interpretation of individual coefficients.\n",
    "\n",
    "The interpretation for each coefficient assumes that all other predictors are held constant.\n",
    "\n",
    "With these caveats in mind, we can say:\n",
    "\n",
    "- Balance appears to have the strongest effect, with each standard deviation increase associated with substantially higher default log-odds.\n",
    "- Income has a much smaller positive coefficient, which may indicate that after controlling for balance, higher-income individuals might take on proportionally more risk.\n",
    "- Student status shows a negative association with default, suggesting lower risk after accounting for balance and income.\n",
    "\n",
    "Unlike our first model where we manually converted coefficients to the original scale, we haven't done this transformation for the pipeline model, making direct comparison with real-world values more difficult. For practical interpretation, we would need to transform these coefficients back to their original scales or consider calculating marginal effects at representative values.\n",
    "\n",
    "Despite these interpretability challenges, the model's predictive performance remains the essential metric, and we observed only marginal improvement with the additional predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Addressing Confounding Effects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "The coefficient for student is negative in the multiple logistic regression result. This is at odds with the single logistic regression result in ISL (pg 142, table 4.2), which shows a positive coefficient for the same (0.4049). That result does align with average balance and default rate by student status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the relationship between student status and balance\n",
    "student_balance = default.groupby('student')['balance'].mean()\n",
    "print(\"Average balance by student status:\")\n",
    "print(student_balance)\n",
    "\n",
    "# Look at default rates by student status\n",
    "default_by_student = default.groupby('student')['default'].apply(\n",
    "    lambda x: (x == 'Yes').mean() * 100\n",
    ")\n",
    "print(\"\\nDefault rate by student status:\")\n",
    "print(default_by_student.map(lambda x: f\"{x:.2f}%\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "When we look at raw default rates without controlling for other factors, as in the single logistic regression model, students appear to have higher default rates overall. This would lead to the incorrect conclusion that being a student directly increases default risk. However, our multiple regression shows the opposite - being a student is associated with a *lower* probability of default when we control for balance and income.\n",
    "\n",
    "This apparent contradiction occurs because of **confounding**, where a variable affects both independent and dependent variables, creating a misleading representation between them. That variable, called a confounder, can make the other two appear related when they are not, or hide a real relationship that exists.\n",
    "\n",
    "1. Credit card balance is a confounding variable - it affects both student status (students tend to have higher balances) and default probability (higher balances increase default risk)\n",
    "2. The negative coefficient reveals that after accounting for balance differences, students are actually *more responsible* borrowers than non-students in comparable financial situations\n",
    "3. Without controlling for balance in our model, the relationship between student status and default risk would be obscured by the confounding effect\n",
    "\n",
    "This example demonstrates why multiple regression is so valuable for causal inference - it helps us isolate the independent effect of each variable by controlling for confounding factors, revealing relationships that might be hidden or misrepresented in simpler analyses.\n",
    "\n",
    "**Note:** We've seen this before - think back to single / linear regression where we were looking at the relationship between sales and advertising. We found that newspaper ad spend was correlated with radio advertising, leading so SLR predictions that radio advertising had a positive effect on sales. But in the MLR case we found the opposite effect. In that case, radio ads are a confounder because it has both a genuine effect on sales, but is also correlated with newspaper advertising. In the SLR case the apparent effect of newspaper ads is actually a side effect of radio ads. Again, MLR makes this clear by controlling for the other variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New section for 2D decision boundary\n",
    "# Use balance and income as features\n",
    "X_2d = default[['balance', 'income']].values\n",
    "y = (default['default'] == 'Yes').astype(int)\n",
    "\n",
    "# Split and scale as before\n",
    "X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n",
    "    X_2d, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler_2d = StandardScaler()\n",
    "X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\n",
    "X_test_2d_scaled = scaler_2d.transform(X_test_2d)\n",
    "\n",
    "# Train model\n",
    "model_2d = LogisticRegression()\n",
    "model_2d.fit(X_train_2d_scaled, y_train_2d)\n",
    "\n",
    "# Create a meshgrid to visualize decision boundary\n",
    "def plot_decision_boundary(X, y, model, scaler):\n",
    "    # Set up the mesh grid\n",
    "    h = 0.01  # Mesh step size\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Scale the mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    mesh_points_scaled = scaler.transform(mesh_points)\n",
    "    \n",
    "    # Predict on the mesh\n",
    "    Z = model.predict(mesh_points_scaled)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    # Plot the data points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, \n",
    "                         edgecolors='k', cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Balance ($)')\n",
    "    plt.ylabel('Income ($)')\n",
    "    plt.title('Decision Boundary for Default Prediction')\n",
    "    plt.colorbar(scatter, label='Default Status')\n",
    "    \n",
    "    # Add contour lines to emphasize boundary\n",
    "    plt.contour(xx, yy, Z, [0.5], linewidths=2, colors='k')\n",
    "    \n",
    "    # Annotate regions\n",
    "    x_center = (x_max + x_min) / 2\n",
    "    y_text_pos = y_max - 0.2 * (y_max - y_min)\n",
    "    # Find a point in each region for text\n",
    "    i, j = np.unravel_index(Z.argmin(), Z.shape)\n",
    "    plt.text(xx[i, j], yy[i, j] - 0.3, \"Predict: No Default\", \n",
    "             ha='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    i, j = np.unravel_index(Z.argmax(), Z.shape)\n",
    "    plt.text(xx[i, j], yy[i, j] - 0.3, \"Predict: Default\", \n",
    "             ha='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(X_2d, y, model_2d, scaler_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
