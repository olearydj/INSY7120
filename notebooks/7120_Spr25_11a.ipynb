{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook introduces the Scikit-Learn interface for classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Load the packages and configure environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Using the Boston data from HW1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data set directly from the web using pandas\n",
    "url = \"https://raw.githubusercontent.com/olearydj/INSY7120/refs/heads/main/notebooks/data/Default.csv\"\n",
    "default = pd.read_csv(url)\n",
    "\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "First, we'll predict `default` based only on credit card `balance`, using logistic regression.\n",
    "\n",
    "We'll use several components from SKL to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Get `X` and `y`. Note that the method below for `y` is exactly equivalent to using `get_dummies` as we've done before:\n",
    "\n",
    "```python\n",
    "y = pd.get_dummies(default_df['default'], drop_first=True, dtype=int)\n",
    "```\n",
    "\n",
    "For this simple binary case, the code below creates a boolean array based on the comparison `default == 'Yes'`. In Python `True` is equivalent to `1` and `False` to zero, and `astype(int)` converts them to that representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature and target\n",
    "X = default[['balance']].values  # We need 2D array for sklearn\n",
    "y = (default['default'] == 'Yes').astype(int)  # Convert to binary 0/1\n",
    "\n",
    "# use value_counts to see class counts\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "There are 10,000 observations, 9667 of which are False (96.7%), and 333 are True (3.3%).\n",
    "\n",
    "For this example, we'll use simple train-test split validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Logistic Regression uses gradient descent, which is sensitive to differences in scale between the features. Normalize them by z-score standardization using `StandardScalar`. Here we are taking care during fit to calculate the scalar parameters ($\\mu$ and $\\sigma$) using only the training data. The results are used to transform both the training and the test data. This avoids leaking information about the test data (e.g. scale) into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scalar using training data AND apply that transformation to the same\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# use the scalar coefficients from training to transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "It is simple to fit the `LogisticRegression` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Finally, make predictions. There are two methods used below. `model.predict` generates labels where `model.predict_proba` generates the probabilities. SKL's binary classifiers use a threshold of 0.5 for label assignment. Observations with a predicted probability of greater than 50% get the positive label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Let's look at the relationship between probabilities, predictions, and the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to show samples with predictions and probabilities\n",
    "\n",
    "# Get a sample of predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': y_pred,\n",
    "    'probability': y_pred_prob\n",
    "})\n",
    "\n",
    "# Sort by probability to show examples around the threshold\n",
    "results_df = results_df.sort_values('probability')\n",
    "\n",
    "# Select informative examples (some below and some above the threshold)\n",
    "threshold_examples = pd.concat([\n",
    "    # Examples just below threshold (predict 0)\n",
    "    results_df[(results_df['probability'] > 0.3) & (results_df['probability'] < 0.5)].head(3),\n",
    "    # Examples just above threshold (predict 1)\n",
    "    results_df[(results_df['probability'] >= 0.5) & (results_df['probability'] < 0.7)].head(3)\n",
    "])\n",
    "\n",
    "# Display with formatting\n",
    "pd.set_option('display.precision', 4)\n",
    "print(\"Prediction Examples (threshold = 0.5):\")\n",
    "print(threshold_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "You should be thinking, \"what if I don't want to use a 50% threshold?\" Good question and we will come back to it. TLDR; SKL doesn't support it directly, but it is an important consideration in model development.\n",
    "\n",
    "Let's summarize the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "You should be thinking, \"what are all those numbers?\" Good question, we'll come back to that. Included now for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Interpreting the Predictions\n",
    "\n",
    "These predictions are summarized above as a **Confusion Matrix**. In the case of binary classification it looks like this:\n",
    "\n",
    "|            |                  |        **Predicted**         |        **Predicted**        |\n",
    "| ---------- | ---------------- | :--------------------------: | :-------------------------: |\n",
    "|            |                  |       **Negative (0)**       |      **Positive (1)**       |\n",
    "| **Actual** | **Negative (0)** | True Negative (TN)<br>(2896) | False Positive (FP)<br>(10) |\n",
    "| **Actual** | **Positive (1)** | False Negative (FN)<br>(70)  | True Positive (TP)<br>(24)  |\n",
    "\n",
    "This table follows the scikit-learn format where:\n",
    "- Rows represent actual values (0=No Default, 1=Default), the training labels\n",
    "- Columns represent predicted values (0=No Default, 1=Default)\n",
    "\n",
    "The four cells correspond to:\n",
    "\n",
    "- **Top-Left (2896)**: True Negatives (TN) - Correctly predicted as \"No Default\"\n",
    "- **Top-Right (10)**: False Positives (FP) - Incorrectly predicted as \"Default\"\n",
    "- **Bottom-Left (70)**: False Negatives (FN) - Incorrectly predicted as \"No Default\"\n",
    "- **Bottom-Right (24)**: True Positives (TP) - Correctly predicted as \"Default\"\n",
    "\n",
    "This provides a complete picture of how the model's predictions align with the actual values, showing where the model succeeds and where it makes errors.\n",
    "\n",
    "How would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Answers - Hide Me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "The data has many more observations with a negative label (default = no) than positive. The first row of the CM shows 2896 + 10 = 2906 actual negatives and only 70 + 24 = 94 actual positives. That's a total of 3,000 observations, which matches the 30% test split we used.\n",
    "\n",
    "If we think only in terms of the number of correct predictions, which are on the diagonal, this does quite well: 2896 + 24 / 3000 = 0.9733 → 97.3% correct.\n",
    "\n",
    "It is also very good at identifying non-defaults (first row): 2896 / (2896 + 10) = 0.9966 → 99.7% correct.\n",
    "\n",
    "Struggles to identify actual defaults (second row): 24 / (70 + 24) = 0.2553 → 25.5% correct.\n",
    "\n",
    "Two types of errors, each with different frequencies and implications:\n",
    "\n",
    "- False Positives (10) - predicting default when there is none\n",
    "- False Negatives (70) - missing actual defaults\n",
    "\n",
    "In these results, False Negatives are 7x more common than False Positives. In a lending business, failing to identify potential defaults (FNs) is typically more costly than incorrectly flagging bad borrowers (FPs). The high number of FNs suggests the model might not adequately address the business need despite seeming very accurate. Poor discriminator for this scenario.\n",
    "\n",
    "Why might this model be biased towards predicting no default?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Some fancy matplotlib work courtesy of Claude... Note that the actual and predicted dots should both appear on the 1.0 and 0.0 lines. They are shown stacked above and below those for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results with better annotations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Create small vertical offsets for clarity\n",
    "offset_actual = 0.02  # Offset for actual values\n",
    "offset_pred = -0.02   # Offset for predicted values\n",
    "\n",
    "# Plot actual values (blue dots)\n",
    "plt.scatter(X_test, y_test + offset_actual, color='blue', alpha=0.5, label='Actual')\n",
    "\n",
    "# Plot predicted values (red dots)\n",
    "plt.scatter(X_test, y_pred + offset_pred, color='red', alpha=0.5, label='Predicted')\n",
    "\n",
    "# Plot the decision boundary\n",
    "balance_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 100).reshape(-1, 1)\n",
    "balance_range_scaled = scaler.transform(balance_range)\n",
    "y_prob = model.predict_proba(balance_range_scaled)[:, 1]\n",
    "plt.plot(balance_range, y_prob, 'g-', label='Probability of Default')\n",
    "plt.axhline(y=0.5, color='k', linestyle='--', label='Decision Boundary')\n",
    "\n",
    "# Create annotation boxes for False Negatives and False Positives\n",
    "# False Negatives (top region)\n",
    "fn_rect = patches.Rectangle((1000, 0.99), 930, 0.06, linewidth=2, edgecolor='r', facecolor='none')\n",
    "plt.gca().add_patch(fn_rect)\n",
    "plt.text(1500, 1.08, \"False Negatives\", color='red', fontsize=16, ha='center')\n",
    "\n",
    "# False Positives (bottom right region)\n",
    "fp_rect = patches.Rectangle((1955, -0.01), 600, 0.06, linewidth=2, edgecolor='r', facecolor='none')\n",
    "plt.gca().add_patch(fp_rect)\n",
    "plt.text(2250, -0.07, \"False Positives\", color='red', fontsize=16, ha='center')\n",
    "\n",
    "# Adjust y-axis to accommodate offsets and annotations\n",
    "plt.ylim(-0.15, 1.15)\n",
    "\n",
    "plt.xlabel('Credit Card Balance')\n",
    "plt.ylabel('Default Probability')\n",
    "plt.title('Logistic Regression: Predicting Default based on Balance')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "We can observe a few things here:\n",
    "\n",
    "- The decision boundary (dashed line) at 0.5 probability corresponds to a balance of approximately \\$2000, where the model transitions from predicting \"No Default\" to \"Default\".\n",
    "- Most accounts with balances below \\$2k have low default probabilities and are classified as non-defaults (red dots at bottom)\n",
    "- Most accounts with balances above \\$2k have high default probabilities and are classified as defaults (red dots at top)\n",
    "- There are several misclassifications evident:\n",
    "  - False negatives (actual = default, prediction = non-default): blue dots at y=1 without matching red dot\n",
    "  - False positives (actual = non-default, prediction = default): blue dots at y=0 without matching red dot\n",
    "\n",
    "Most FNs occur with balances between ≈\\$1k and 2k. FPs are less common and occur at higher balances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Interpret Model Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Extract model coefficients from the fitted model and interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model coefficients\n",
    "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
    "print(f\"Coefficient for balance: {model.coef_[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "These don't match the results in ISL Chapter 4:\n",
    "\n",
    "![](images/isl-tbl-4.1.png)\n",
    "\n",
    "Why? We've standardized the features so they are in the scale of standard deviations of the feature values. Where ISL's result is interpreted in terms of $1 changes to the balance, our results must be interpreted as follows:\n",
    "\n",
    "> For every one *standard deviation* increase in `balance`, the *log-odds* of `default` increase by 2.6678."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "If we want to interpret the coefficients in the original scale, we need to reverse the effects of the `StandardScalar`. Here's the mathematical approach:\n",
    "\n",
    "#### The Standardization Process\n",
    "\n",
    "A feature $X$ is standardized using:\n",
    "\n",
    "$$X_{scaled} = \\frac{X - \\mu_X}{\\sigma_X}$$\n",
    "\n",
    "Where $\\mu_X$ is the mean and $\\sigma_X$ is the standard deviation of $X$.\n",
    "\n",
    "#### Logistic Regression with Standardized Data\n",
    "\n",
    "For a model fit on standardized data:\n",
    "\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{scaled}$$\n",
    "\n",
    "Substituting the standardization equation:\n",
    "\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\frac{X - \\mu_X}{\\sigma_X}$$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = \\left[\\beta_0 - \\beta_1\\frac{\\mu_X}{\\sigma_X}\\right] + \\frac{\\beta_1}{\\sigma_X}X$$\n",
    "\n",
    "#### Coefficients in Original Scale\n",
    "\n",
    "From this rearrangement, we can see:\n",
    "\n",
    "1. The coefficient in original scale: $\\beta_{1,orig} = \\frac{\\beta_1}{\\sigma_X}$\n",
    "\n",
    "2. The intercept in original scale: $\\beta_{0,orig} = \\beta_0 - \\beta_1\\frac{\\mu_X}{\\sigma_X}$\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "After transformation, we can properly interpret:\n",
    "- For each 1-unit increase in the original feature, the log-odds increase by $\\beta_{1,orig}$\n",
    "- When the original feature equals zero, the log-odds equal $\\beta_{0,orig}$\n",
    "\n",
    "This manual transformation ensures we maintain the statistical benefits of standardization during model training while enabling interpretable coefficients in the original units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scaling parameters from the scaler\n",
    "# first index, only one feature\n",
    "std_dev_balance = scaler.scale_[0]\n",
    "mean_balance = scaler.mean_[0]\n",
    "\n",
    "# Convert the coefficient back to the original scale\n",
    "coef_original = model.coef_[0][0] / std_dev_balance\n",
    "\n",
    "# Convert the intercept back to the original scale\n",
    "intercept_original = model.intercept_[0] - (model.coef_[0][0] * mean_balance / std_dev_balance)\n",
    "\n",
    "print(f\"Standard deviation of balance: {std_dev_balance:.4f}\")\n",
    "print(f\"Mean of balance: {mean_balance:.4f}\")\n",
    "print(f\"Coefficient in original scale: {coef_original:.4f}\")\n",
    "print(f\"Intercept in original scale: {intercept_original:.4f}\")\n",
    "print(f\"For every $1 increase in balance, log-odds of default increases by {coef_original:.4f}\")\n",
    "print(f\"The log-odds of default when balance = 0 is {intercept_original:.4f}\")\n",
    "\n",
    "# The equation in the original scale would be:\n",
    "print(\"\\nLogistic regression equation in original scale:\")\n",
    "print(f\"log-odds(default) = {intercept_original:.4f} + {coef_original:.4f} × balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "These results are close to those obtained by ISL (-10.6513 + 0.0055 x balance). The difference in the intercept could come down to some combination of the splitting method, randomization, precision limits, and optimization method used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Also, introducing SKL pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['student']\n",
    "numerical_cols = ['balance', 'income']\n",
    "\n",
    "# Create preprocessor for mixed data types\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with preprocessing and logistic regression\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Prepare X and y\n",
    "X = default[numerical_cols + categorical_cols]\n",
    "y = (default['default'] == 'Yes').astype(int)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the model pipeline\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "y_pred_prob = model_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Extract and interpret coefficients\n",
    "# Get the column names after one-hot encoding\n",
    "encoded_feature_names = numerical_cols + ['student_Yes']\n",
    "\n",
    "# Get the coefficients from the model\n",
    "coefficients = model_pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Create a DataFrame to display the coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': encoded_feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(\"\\nModel Coefficients:\")\n",
    "print(coef_df)\n",
    "\n",
    "# Calculate the intercept in original scale\n",
    "intercept = model_pipeline.named_steps['classifier'].intercept_[0]\n",
    "print(f\"\\nIntercept: {intercept:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Note: only one prediction changes from FN to TP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
