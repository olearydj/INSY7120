{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Finishing our discussion of Tree Based methods.\n",
    "\n",
    "But first..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Feature Selection / Extraction Methods\n",
    "\n",
    "Alternatives to sequential other feature selection methods we discussed: procedural (best / forward / backward select) and algorithmic (regularization in MLR, feature importance in d-trees). Not functionally the same, but used to answer the same question: how do we simplify the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a synthetic dataset with built-in correlations and redundant features\n",
    "def generate_sample_data(n_samples=1000, n_features=20, n_informative=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset with some informative and some redundant features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    n_features : int\n",
    "        Total number of features to generate\n",
    "    n_informative : int\n",
    "        Number of features that are informative\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        The generated feature matrix\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        The generated target vector\n",
    "    feature_names : list\n",
    "        Names for each feature\n",
    "    \"\"\"\n",
    "    # Create a classification problem\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_informative,\n",
    "        n_redundant=int(n_features * 0.3),  # 30% redundant features\n",
    "        n_repeated=0,\n",
    "        n_classes=2,\n",
    "        n_clusters_per_class=2,\n",
    "        weights=None,\n",
    "        flip_y=0.05,\n",
    "        class_sep=1.0,\n",
    "        hypercube=True,\n",
    "        shift=0.0,\n",
    "        scale=1.0,\n",
    "        shuffle=True,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = [f'feature_{i+1}' for i in range(n_features)]\n",
    "    \n",
    "    # Add some additional correlated features to make PCA more interesting\n",
    "    if n_features > 5:\n",
    "        # Make feature_5 correlated with feature_1 and feature_2\n",
    "        X[:, 4] = 0.7 * X[:, 0] + 0.3 * X[:, 1] + 0.2 * np.random.randn(n_samples)\n",
    "        \n",
    "        # Make feature_6 correlated with feature_3\n",
    "        X[:, 5] = 0.85 * X[:, 2] + 0.15 * np.random.randn(n_samples)\n",
    "    \n",
    "    print(f\"Generated dataset with {n_samples} samples and {n_features} features\")\n",
    "    print(f\"Class distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    return X, y, feature_names\n",
    "\n",
    "# Generate the data\n",
    "X, y, feature_names = generate_sample_data()\n",
    "\n",
    "# Print some basic information\n",
    "print(\"\\nFeature matrix shape:\", X.shape)\n",
    "print(\"Target vector shape:\", y.shape)\n",
    "print(\"First 5 feature names:\", feature_names[:5])\n",
    "\n",
    "# Calculate feature correlations to show the redundancy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a correlation matrix\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "correlation_matrix = df.corr().abs()\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', \n",
    "            xticklabels=feature_names, yticklabels=feature_names)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nYou can now use this data with the PCA and SelectKBest examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Choosing the \"best\" features in the dataset. Everything we've done so far falls into this category - doesn't make new features, just helps us choose / identify the most \"important.\"\n",
    "\n",
    "`SelectKBest` is another alternative from SKL. It selects features based on statistical tests of relationship (e.g., ANOVA F-test, Chi-squared) with the target. This bridges the gap between the inferential and predictive approaches we've discussed.\n",
    "\n",
    "The basic implemenation is given below, using ANOVA (`f_classif`). This measures the ratio ($F$) of between-class and within-class variance, so higher F-values indicate features that better separate the classes. Compares the means of different classes for each feature. Refer to your stats class notes for more info! ;-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Create and fit SelectKBest for classification\n",
    "k = 5  # Select top 5 features\n",
    "selector = SelectKBest(f_classif, k=k)  \n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "selected_features = [feature_names[i] for i in selected_indices]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Here is some code to extract the scores and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores and p-values\n",
    "scores = selector.scores_\n",
    "pvalues = selector.pvalues_\n",
    "\n",
    "# Create a DataFrame with all scores and selection status\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Score': scores,\n",
    "    'p-value': pvalues,\n",
    "    'Selected': np.isin(np.arange(len(feature_names)), selected_indices)\n",
    "})\n",
    "\n",
    "# Sort by score in descending order\n",
    "feature_scores = feature_scores.sort_values('Score', ascending=False)\n",
    "\n",
    "# Visualization 1: Bar chart of feature scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(\n",
    "    feature_scores['Feature'], \n",
    "    feature_scores['Score'],\n",
    "    color=[('steelblue' if selected else 'lightgray') for selected in feature_scores['Selected']]\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(f'Feature Importance Scores (Top {k} Selected)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('F-Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Key Characteristics\n",
    "\n",
    "1. Univariate Analysis: Each feature is evaluated independently, ignoring interactions\n",
    "2. Quick Computation: Much faster than wrapper methods like RFE\n",
    "3. No Model Training: Doesn't require training a model to evaluate features\n",
    "4. Statistical Foundation: Based on well-established statistical tests\n",
    "\n",
    "The key limitation is that `SelectKBest` doesn't consider feature interactions or redundancy. Two highly correlated features might both be selected even though they provide similar information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Unlike feature *selection*, feature *extraction* creates **new features** by transforming the original ones. Several such methods exist, including factor analysis, t-SNE, and UMAP. Here, we'll focus on Principal Component Analysis (PCA).\n",
    "\n",
    "PCA transforms existing features into uncorrelated components ordered by the amount of variance they explain. It works by finding orthogonal directions (eigenvectors) in the feature space where data varies the most, and projects the data onto these directions. See also, this [wikipedia page on PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) and related links. You may remember eigenvalues and eigenvectors from linear algebra (I didn't).\n",
    "\n",
    "The resulting principal components are linear combinations of original features, weighted by their importance.\n",
    "\n",
    "The basic SKL implementation follows. Note: always standardize your data first, since PCA is sensitive to feature scales! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Always scale data before PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create and fit PCA\n",
    "pca = PCA(n_components=5)  # or n_components=0.95 to retain 95% variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Access explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(explained_variance)\n",
    "print(sum(explained_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Five components that explain 65.6% of the variance in the feature space.\n",
    "\n",
    "But what do they represent? Harder to answer - feature loadings give us the contributions of each original feature on a principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# 3. Feature loadings (coefficients)\n",
    "n_components = 5  # Examine the first five components\n",
    "loadings = pd.DataFrame(\n",
    "    data=pca.components_[:n_components, :].T,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "    index=feature_names\n",
    ")\n",
    "\n",
    "# Create a heatmap of feature loadings\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(loadings, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Feature Loadings (Coefficients) for First Five Principal Components')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "From this, if we understand the data we may be able to intuit what each component is \"capturing\" about the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Key Characteristics\n",
    "\n",
    "1. Unsupervised Transformation: Creates new features (principal components) based solely on input feature relationships, without considering the target variable\n",
    "2. Orthogonality: Produces completely uncorrelated components, eliminating multicollinearity issues\n",
    "3. Variance Maximization: Orders components by explained variance, allowing dimensionality reduction with minimal information loss\n",
    "4. Linear Combinations: Creates components as weighted linear combinations of original features\n",
    "\n",
    "The key limitation is that PCA components lose direct interpretability since they're combinations of original features. The transformation is also sensitive to feature scaling, so standardization is essential before applying PCA. Additionally, PCA assumes linear relationships and is less effective when nonlinear patterns dominate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Packaged Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Random Forests (Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Implementation is straightforward, as you have come to expect.\n",
    "\n",
    "Critical parameters for `RandomForestClassifier` include:\n",
    "\n",
    "- `n_estimators`: Number of trees (diminishing returns, convergence properties)\n",
    "- `max_features`: Number of features considered for each split\n",
    "  - Classification default: `sqrt(n_features)`\n",
    "  - Regression default: `n_features`\n",
    "- `max_depth`: How deep to grow trees (depth vs. generalization)\n",
    "- `min_samples_split` / `min_samples_leaf`: Controlling tree complexity\n",
    "- `bootstrap`: whether to use bootstrap sampling\n",
    "- `oob_score`: Whether to use out-of-bag samples for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                          n_informative=15, n_redundant=5, \n",
    "                          random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                   random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features='sqrt',\n",
    "                           max_depth=None, min_samples_split=2,\n",
    "                           bootstrap=True, oob_score=True,\n",
    "                           random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Training accuracy: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test accuracy: {rf.score(X_test, y_test):.4f}\")\n",
    "print(f\"OOB accuracy: {rf.oob_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Gradient Boosting (Boosting)\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "- `learning_rate`: Controls contribution of each tree\n",
    "- `n_estimators`: Number of sequential trees\n",
    "- `subsample`: Fraction of samples for stochastic gradient boosting (< 1.0)\n",
    "- `max_depth`: Depth of individual trees (usually shallow, 3-5 levels)\n",
    "- `min_samples_split` / `min_samples_leaf`: Controls tree complexity\n",
    "- `max_features`: Number of features for each split (similar to Random Forest)\n",
    "\n",
    "In practice, the full value of each error prediction is not used. Some portion of it, called the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                               max_depth=3, subsample=0.8,\n",
    "                               random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Training accuracy: {gb.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test accuracy: {gb.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Staged predictions (analyzing learning curve)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# For each iteration/stage of the boosting process:\n",
    "for y_pred_train in gb.staged_predict(X_train):\n",
    "    train_scores.append(accuracy_score(y_train, y_pred_train))\n",
    "    \n",
    "for y_pred_test in gb.staged_predict(X_test):\n",
    "    test_scores.append(accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_scores) + 1), train_scores, label='Training')\n",
    "plt.plot(range(1, len(test_scores) + 1), test_scores, label='Testing')\n",
    "plt.xlabel('Number of Boosting Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Gradient Boosting Learning Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### State of the Art in Gradient Boosting\n",
    "\n",
    "While the SKL implementation is adequate, for the state of the art in Gradient Boosting, use either `XGBoost` ([eXtreme Gradient Boosting](https://xgboost.ai/), by Tianqi Chen of DLMC) or `LightGBM` ([LightGBM docs](https://lightgbm.readthedocs.io/en/stable/), by Microsoft). Both build on baseline GBM implementations like SKL's with different performance and efficiency optimizations. In short, XGBoost, which has built-in regularization and handles missing values automatically, is best when accuracy is critical and training time isn't a bottleneck. LightGBM is more performant and handles categorical features better. It is preferred for large data sets or other situations where training time is a major concern.\n",
    "\n",
    "It is reasonable to claim that, in many cases, the best single model available for tabular data is XGBoost. It is a strong default choice with excellent performance across a wide range of scenarios.\n",
    "\n",
    "The cell below shows a basic implementation of XGB with the SKL interface using the same data that we just trained the RF and GBM on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "As we'll see, the key to success with XGBoost is proper parameter tuning, which often requires experimentation based on your specific dataset. Key parameters include:\n",
    "\n",
    "- `n_estimators`: Number of trees (boosting rounds).\n",
    "- `max_depth`: Maximum depth of each tree.\n",
    "- `learning_rate`: Step size shrinkage used to prevent overfitting.\n",
    "- `subsample`: Fraction of samples used for fitting individual trees.\n",
    "- `colsample_bytree`: Fraction of features used for building each tree.\n",
    "- `gamma`: Minimum loss reduction required for a split.\n",
    "- `reg_alpha`: L1 regularization term on weights.\n",
    "- `reg_lambda`: L2 regularization term on weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create and train model\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)  # Predict on training data\n",
    "y_pred_test = model.predict(X_test)    # Predict on test data\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Training accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "In this case, XGB appears to have made me a liar. Test accuracy is lower than wasy previously measured with either RF or GBM.\n",
    "\n",
    "Of course, for a more representative result, this should be compared with `GridSearchCV` or `RandomizedSearchCV` to optimize the hyperparameters and evaluate the best model on the test data. Below we'll explore a large hyperparameter space ($4 \\times 5 \\times 4 \\times 3 \\times 3 \\times 4 \\times 3 \\times 4 \\times 4 = 92,160$ parameter combinations) for the optimal solution. Using an exhaustive grid search with 5-fold cross-validation would fit **460,800 models!** By using random search we fit only 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import time\n",
    "\n",
    "# 1. Define a wide parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'reg_lambda': [0.1, 0.5, 1, 5]\n",
    "}\n",
    "\n",
    "# 2. First evaluate baseline model with cross-validation\n",
    "base_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "base_cv_scores = cross_val_score(base_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Baseline CV Accuracy: {base_cv_scores.mean():.4f} ± {base_cv_scores.std():.4f}\")\n",
    "\n",
    "# 3. RandomizedSearchCV (more efficient for large parameter spaces)\n",
    "start_time = time.time()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBClassifier(objective='binary:logistic', random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,  # Number of parameter settings sampled\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "random_search_time = time.time() - start_time\n",
    "print(f\"RandomizedSearchCV completed in {random_search_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV accuracy: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Test the best model from random search\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(f\"Test accuracy with best random search model: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Test accuracy with best grid search model: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Compare all approaches\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(f\"Baseline model test accuracy: {accuracy_score(y_test, base_model.fit(X_train, y_train).predict(X_test)):.4f}\")\n",
    "print(f\"RandomizedSearchCV best test accuracy: {accuracy_score(y_test, random_search.predict(X_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "This improves our test accuracy to 0.90, a significant improvement over the baseline - enough to surpass the default performance (an unfair comparison) of both RF and GBM above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation for Ensemble Evaluation\n",
    "\n",
    "When working with ensemble methods that require hyperparameter tuning, standard cross-validation can lead to optimistically biased performance estimates. Nested cross-validation provides a more honest assessment by separating model selection from model evaluation.\n",
    "\n",
    "### The Problem with Simple CV for Ensembles\n",
    "\n",
    "With simple CV and hyperparameter tuning, we:\n",
    "1. Split data into K folds\n",
    "2. Try different hyperparameters and select the best based on CV performance\n",
    "3. Report this best performance as our estimate\n",
    "\n",
    "This can lead to overfitting to the validation set because the hyperparameters were specifically chosen to maximize performance on that data.\n",
    "\n",
    "### Nested CV Solution\n",
    "\n",
    "Nested cross-validation uses two loops:\n",
    "- An outer loop that splits data into training and test sets multiple times\n",
    "- An inner loop that performs hyperparameter tuning using only the training portion from each outer split\n",
    "\n",
    "This creates a clear separation between:\n",
    "1. Model selection (finding optimal hyperparameters in the inner loop)\n",
    "2. Model evaluation (assessing performance on truly unseen data in the outer loop)\n",
    "\n",
    "For ensemble methods, this is particularly important because they have many hyperparameters that can be tuned to maximize performance. Without nested CV, we risk selecting hyperparameter combinations that happen to work well on our validation data but don't generalize to new data.\n",
    "\n",
    "Here's a practical implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Outer cross-validation loop\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Model and parameter grid\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Nested CV scores\n",
    "nested_scores = []\n",
    "\n",
    "# For each outer training/testing split\n",
    "# the split() method returns a list of indicies that are used to extract train and test values for the features and targets\n",
    "for train_idx, test_idx in outer_cv.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Inner CV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, param_grid=param_grid,\n",
    "        cv=inner_cv, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate best model from inner CV on outer test fold\n",
    "    best_model = grid_search.best_estimator_\n",
    "    score = best_model.score(X_test, y_test)\n",
    "    nested_scores.append(score)\n",
    "    \n",
    "    print(f\"Outer fold: Best params {grid_search.best_params_}, Test score: {score:.4f}\")\n",
    "\n",
    "# Overall performance estimate\n",
    "print(f\"Nested CV accuracy: {np.mean(nested_scores):.4f} ± {np.std(nested_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "This output shows that each outer loop uses a fold to test the performance of the best model found by the grid search, which uses its own CV to score each hyperparameter set. The Nested CV accuracy is based on the runs recorded. It is used to assess the overall model stability, *not* as a way to choose the best model. As we can see, the models selected by the first and last outer fold have identical parameter sets. The same is true for the second and fourth.\n",
    "\n",
    "This process is typically used as a precursor to final model training. The key insight is that nested CV doesn't directly give you the \"best\" hyperparameters - it gives you an unbiased estimate of performance and shows you how stable your hyperparameter selection process is across different data subsets.\n",
    "\n",
    "Typically, the next step is to run a final grid search on all data to find the best parameters for your final model, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After completing nested CV evaluation (as shown in previous example)\n",
    "print(f\"Nested CV accuracy: {np.mean(nested_scores):.4f} ± {np.std(nested_scores):.4f}\")\n",
    "\n",
    "# Now train the final model using all data\n",
    "print(\"\\n--- Training Final Deployment Model ---\")\n",
    "\n",
    "# Create the final grid search on all data\n",
    "final_grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # Same inner CV strategy \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on ALL data (not just a training subset)\n",
    "final_grid_search.fit(X, y)\n",
    "\n",
    "# Show best parameters found using all available data\n",
    "print(f\"Final model best parameters: {final_grid_search.best_params_}\")\n",
    "print(f\"Final model CV score: {final_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "final_model = RandomForestClassifier(\n",
    "    **final_grid_search.best_params_,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train on all data\n",
    "final_model.fit(X, y)\n",
    "\n",
    "print(\"Final model trained and ready for deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The final model is fit using all available data because we're not using it to predict performance or guide model development. We are willing to accept its output because we validated its performance using the prior steps.\n",
    "\n",
    "When deployed, `final_model` will be used to generate predictions for new observations.\n",
    "\n",
    "Key Benefits for Ensemble Evaluation:\n",
    "\n",
    "1. Unbiased performance estimation: Each performance score comes from data that was not used for hyperparameter selection\n",
    "2. Reliable model comparison: Properly compare complex ensembles against simpler models\n",
    "3. Reduced risk of overfitting: Avoid overestimating the performance of heavily tuned ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Custom Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "This is an open-ended topic, as it covers an arbitrary combination of models. This notebook can only scratch the surface with an introduction and some starter code.\n",
    "\n",
    "The key question is how to combine the results of multiple models. Two common approaches are voting and stacking.\n",
    "\n",
    "### Voting Methods\n",
    "\n",
    "As the name suggests, voting uses self-described methods to generate predictions for the ensemble. For classification problems using `VotingClassifier`, this can be based on simple majority (hard voting) or weighted average of predicted probabilities (soft voting). Likewise, `VotingRegressor` can use simple or weighted averages of the predicted values.\n",
    "\n",
    "A sample implementation is provided below using a combination of Logistic Regression, Random Forest, and Support Vector Machine (covered in 7130) classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create three base models\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "svc_clf = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Create and train voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rf_clf), ('svc', svc_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Compare individual models with ensemble\n",
    "for clf, label in zip([log_clf, rf_clf, svc_clf, voting_clf],\n",
    "                      ['Logistic Regression', 'Random Forest', 'SVM', 'Voting']):\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"{label} test accuracy: {clf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "As you can see, the voting test accuracy is higher than most individual models, but not all. Remember that this is train-test validation of an often-misleading performance metric.\n",
    "\n",
    "For any ensemble it is best to include a diverse but complementary set of models - ideally each has different strengths / weaknesses to provide the best mix of coverage. The rationale for this should be self-evident. One way to evaluate that is by analyzing the correlation between model predictions, as seen below. Other methods exist, including agreement / disagreement analysis and Cohen's Kappa Matrix (a measure of agreement beyond chance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from each base model\n",
    "logistic_preds = log_clf.predict(X_test)\n",
    "rf_preds = rf_clf.predict(X_test)\n",
    "svc_preds = svc_clf.predict(X_test)\n",
    "\n",
    "# Create a DataFrame of predictions\n",
    "pred_df = pd.DataFrame({\n",
    "    'Logistic': logistic_preds,\n",
    "    'RandomForest': rf_preds,\n",
    "    'SVM': svc_preds\n",
    "})\n",
    "\n",
    "# 1. Correlation matrix of predictions\n",
    "corr_matrix = pred_df.corr()\n",
    "print(\"Prediction correlation matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation between Model Predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Stacking Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Stacking uses the outputs of the ensemble as inputs for a meta-model that generates the final prediction. Unsurprisingly, it is implemented with SKL's `StackingClassifier` and `StackingRegressor`. Note that stacking requires CV to prevent overfitting in the meta-model. The process involves training each base model on $K-1$ folds before generating predictions for the held-out data (remaining fold). That way the predictions used as inputs for the meta-model are based on unseen data, and represent generalized performance of the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svc', SVC(probability=True, random_state=42))\n",
    "]\n",
    "\n",
    "# Meta-model\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Stacking classifier with CV\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "print(f\"Stacking test accuracy: {stacking_clf.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Compare with base models\n",
    "for name, clf in base_models:\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"{name} test accuracy: {clf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Unlike with the voting result, the stacked ensemble outperforms all base models. This performance improvement is due to a combination of both the intrinsic cross-validation and the model architecture. While voting uses fixed weights or simple majority, stacking's meta-model learns optimal weights for combining base model predictions. This allows it to give more influence to models that perform better in specific regions of the feature space. It does so by learning patterns that the simple vote can't capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### SKL Pipelines for Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define base models for stacking\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svc', SVC(probability=True, random_state=42))\n",
    "]\n",
    "\n",
    "# Create stacking ensemble\n",
    "stacking_ensemble = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LogisticRegression(random_state=42),\n",
    "    cv=5,\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and stacking ensemble\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('stacking', stacking_ensemble)\n",
    "])\n",
    "\n",
    "# Train and evaluate\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(f\"Pipeline test accuracy: {pipeline.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'pca__n_components': [0.85, 0.9, 0.95],\n",
    "    'stacking__rf__n_estimators': [50, 100],\n",
    "    'stacking__gb__learning_rate': [0.05, 0.1],\n",
    "    'stacking__final_estimator__C': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test accuracy with best params: {grid_search.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Compare with individual models through pipeline for fair comparison\n",
    "base_pipelines = {}\n",
    "for name, clf in base_models:\n",
    "    base_pipelines[name] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=0.95)),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    base_pipelines[name].fit(X_train, y_train)\n",
    "    print(f\"{name} pipeline test accuracy: {base_pipelines[name].score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Here the best ensemble achieves an accuracy of 0.9400, which is slightly lower than that for the base pipeline (0.9450). If it were easy everyone would be doing it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Accuracy is Not Enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Remember to thoroughly interrogate your results. The following comprehensive code block demonstrates a number of methods that you can use to do so. Most we have discussed, a few we have not (primarily McNemar's test, which evaluates the significant difference between models). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Create a collection of models to compare\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Voting Ensemble': voting_clf,  # From your previous example\n",
    "    'Stacking Ensemble': stacking_clf  # From your previous example\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "print(\"Performance Comparison Against Single Models\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate metrics for each model\n",
    "for name, model in models.items():\n",
    "    if name not in ['Voting Ensemble', 'Stacking Ensemble']:  # These are already fitted\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Calculate ROC AUC if the model supports predict_proba\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        results[name]['roc_auc'] = roc_auc_score(y_test, y_prob)\n",
    "    else:\n",
    "        results[name]['roc_auc'] = None\n",
    "    \n",
    "    # Print basic results\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"Accuracy:  {results[name]['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {results[name]['precision']:.4f}\")\n",
    "    print(f\"Recall:    {results[name]['recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {results[name]['f1']:.4f}\")\n",
    "    if results[name]['roc_auc'] is not None:\n",
    "        print(f\"ROC AUC:   {results[name]['roc_auc']:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Create DataFrame for easy comparison\n",
    "metrics_df = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'Accuracy': results[model_name]['accuracy'],\n",
    "        'Precision': results[model_name]['precision'],\n",
    "        'Recall': results[model_name]['recall'],\n",
    "        'F1 Score': results[model_name]['f1'],\n",
    "        'ROC AUC': results[model_name]['roc_auc']\n",
    "    }\n",
    "    for model_name in results.keys()\n",
    "}).T  # Transpose for better display\n",
    "\n",
    "print(\"\\nMetrics Summary:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Visualization 1: Bar chart comparison of metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "metrics_df[['Accuracy', 'Precision', 'Recall', 'F1 Score']].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: ROC Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {results[name][\"roc_auc\"]:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Statistical Significance Testing with McNemar's test\n",
    "print(\"\\nStatistical Significance Testing (McNemar's Test):\")\n",
    "print(\"H0: Two models have the same error rate\")\n",
    "print(\"p-value < 0.05 indicates statistically significant difference\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "model_names = list(models.keys())\n",
    "p_values = np.zeros((len(model_names), len(model_names)))\n",
    "\n",
    "for i, model1 in enumerate(model_names):\n",
    "    for j, model2 in enumerate(model_names):\n",
    "        if i != j:\n",
    "            # Get predictions\n",
    "            y_pred1 = models[model1].predict(X_test)\n",
    "            y_pred2 = models[model2].predict(X_test)\n",
    "            \n",
    "            # Create contingency table with counts of:\n",
    "            # [0,0]: both models correct\n",
    "            # [0,1]: model1 correct, model2 incorrect\n",
    "            # [1,0]: model1 incorrect, model2 correct\n",
    "            # [1,1]: both models incorrect\n",
    "            table = [\n",
    "                [(y_pred1 == y_test) & (y_pred2 == y_test), (y_pred1 == y_test) & (y_pred2 != y_test)],\n",
    "                [(y_pred1 != y_test) & (y_pred2 == y_test), (y_pred1 != y_test) & (y_pred2 != y_test)]\n",
    "            ]\n",
    "            table = np.array([[np.sum(cell) for cell in row] for row in table])\n",
    "            \n",
    "            # Calculate McNemar's test\n",
    "            if table[0, 1] + table[1, 0] > 0:  # Check if there are any discordant predictions\n",
    "                # Use the correct function name for newer scipy versions\n",
    "                try:\n",
    "                    # Try newer scipy version method\n",
    "                    p_values[i, j] = stats.binomtest(\n",
    "                        k=table[0, 1],\n",
    "                        n=table[0, 1] + table[1, 0],\n",
    "                        p=0.5\n",
    "                    ).pvalue\n",
    "                except AttributeError:\n",
    "                    # Fall back to older version if available\n",
    "                    p_values[i, j] = stats.binom_test(\n",
    "                        x=table[0, 1],\n",
    "                        n=table[0, 1] + table[1, 0],\n",
    "                        p=0.5\n",
    "                    )\n",
    "            else:\n",
    "                p_values[i, j] = 1.0  # No difference if no discordant predictions\n",
    "\n",
    "# Create p-value matrix\n",
    "p_value_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
    "print(p_value_df.round(4))\n",
    "\n",
    "# Visualization of p-values (highlight significant differences)\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.zeros_like(p_values, dtype=bool)\n",
    "np.fill_diagonal(mask, True)  # Mask the diagonal\n",
    "\n",
    "sns.heatmap(\n",
    "    p_value_df, \n",
    "    annot=True, \n",
    "    fmt='.4f', \n",
    "    cmap='coolwarm_r', \n",
    "    mask=mask,\n",
    "    vmin=0, \n",
    "    vmax=0.1,\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title(\"McNemar's Test p-values (< 0.05 indicates significant difference)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Learning Curves - How performance varies with training data size\n",
    "def plot_learning_curve(estimator, title, X, y, cv=5, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Generate learning curves for each model (except ensemble models which are already fitted)\n",
    "for name, model in models.items():\n",
    "    if name not in ['Voting Ensemble', 'Stacking Ensemble']:  # Skip already fitted models\n",
    "        plot_learning_curve(\n",
    "            model, f'Learning Curve - {name}', X, y, cv=5, n_jobs=-1\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "From this you should be able to conclude that ensemble methods (particularly stacking) provide the best overall performance, though the difference between Random Forest and the ensembles is not statistically significant. Gradient Boosting shows strong potential but would likely benefit from more data. The Decision Tree serves as a good baseline but is clearly inadequate for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Cost-Performance Tradeoffs\n",
    "\n",
    "In practice model selection involves balancing multiple factors:\n",
    "\n",
    "- Performance (accuracy, precision, recall)\n",
    "- Computational costs (training time, prediction time)\n",
    "- Model complexity and interpretability\n",
    "- Resource constraints in deployment settings\n",
    "\n",
    "The following, final code chunk demonstrates these trades.\n",
    "\n",
    "Note: some of the timing data here is estimated, so only the general relationships are illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n\\n### Model Complexity vs. Performance Trade-offs ###\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dictionary to track complexity and timing\n",
    "model_complexity = {\n",
    "    'Decision Tree': {'n_params': 1, 'complexity': 'Low'},  \n",
    "    'Random Forest': {'n_params': 100, 'complexity': 'Medium'},  # 100 trees\n",
    "    'Gradient Boosting': {'n_params': 100, 'complexity': 'Medium-High'},  # 100 trees, sequential\n",
    "    'Voting Ensemble': {'n_params': 3, 'complexity': 'High'},  # 3 base models\n",
    "    'Stacking Ensemble': {'n_params': 4, 'complexity': 'Very High'}  # 3 base models + meta-model\n",
    "}\n",
    "\n",
    "# Collect timing information\n",
    "timing_results = {}\n",
    "\n",
    "# Test models for timing (training and prediction)\n",
    "for name, model in models.items():\n",
    "    if name not in ['Voting Ensemble', 'Stacking Ensemble']:  # Skip already trained models\n",
    "        # Time training\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Time prediction\n",
    "        start_time = time.time()\n",
    "        model.predict(X_test)\n",
    "        predict_time = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        timing_results[name] = {\n",
    "            'training_time': train_time,\n",
    "            'prediction_time': predict_time,\n",
    "            'accuracy': results[name]['accuracy']\n",
    "        }\n",
    "\n",
    "# For pre-trained ensemble models, just time prediction\n",
    "for name in ['Voting Ensemble', 'Stacking Ensemble']:\n",
    "    if name in models:\n",
    "        # Time prediction only\n",
    "        start_time = time.time()\n",
    "        models[name].predict(X_test)\n",
    "        predict_time = time.time() - start_time\n",
    "        \n",
    "        # Estimate training time based on complexity (relative to base models)\n",
    "        if name == 'Voting Ensemble':\n",
    "            # Roughly sum of base models\n",
    "            train_time = sum([timing_results[m]['training_time'] \n",
    "                             for m in ['Random Forest', 'Gradient Boosting', 'Decision Tree']]) * 1.1\n",
    "        else:  # Stacking\n",
    "            # Sum of base models + overhead for meta-model training with CV\n",
    "            train_time = sum([timing_results[m]['training_time'] \n",
    "                             for m in ['Random Forest', 'Gradient Boosting', 'Decision Tree']]) * 1.5\n",
    "        \n",
    "        # Store results\n",
    "        timing_results[name] = {\n",
    "            'training_time': train_time,\n",
    "            'prediction_time': predict_time,\n",
    "            'accuracy': results[name]['accuracy']\n",
    "        }\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "timing_df = pd.DataFrame(timing_results).T\n",
    "timing_df['model'] = timing_df.index\n",
    "timing_df['complexity'] = timing_df.index.map(lambda x: model_complexity[x]['complexity'])\n",
    "timing_df['n_params'] = timing_df.index.map(lambda x: model_complexity[x]['n_params'])\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel Performance and Computational Cost:\")\n",
    "print(timing_df[['complexity', 'training_time', 'prediction_time', 'accuracy']])\n",
    "\n",
    "# Visualization of trade-offs\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Create bubble chart - x: training time, y: accuracy, size: model complexity\n",
    "plt.scatter(timing_df['training_time'], \n",
    "           timing_df['accuracy'], \n",
    "           s=timing_df['n_params']*5,  # Size based on number of parameters\n",
    "           alpha=0.7)\n",
    "\n",
    "# Add labels for each point\n",
    "for i, model in enumerate(timing_df.index):\n",
    "    plt.annotate(model, \n",
    "                (timing_df['training_time'][i], timing_df['accuracy'][i]),\n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.title('Model Performance vs. Training Time')\n",
    "plt.xlabel('Training Time (seconds)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Bubble size represents relative model complexity.\\n\")\n",
    "\n",
    "# Create bar chart for prediction time\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.barh(timing_df.index, timing_df['prediction_time'], color='skyblue')\n",
    "plt.xlabel('Prediction Time (seconds)')\n",
    "plt.title('Model Prediction Time Comparison')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Key Trade-off Insights:\n",
    "\n",
    "1. Complexity vs. Performance: While more complex models generally perform better, the\n",
    "   improvement from Random Forest to Stacking Ensemble may not justify the increased\n",
    "   complexity and computational cost for all applications.\n",
    "2. Training vs. Prediction Time: Ensemble methods take significantly longer to train but\n",
    "   their prediction time may be acceptable for many applications.\n",
    "3. Diminishing Returns: There appears to be a point of diminishing returns where additional\n",
    "   complexity yields only marginal performance gains.\n",
    "4. Recommended Approach: For this dataset, Random Forest offers the best balance of\n",
    "   performance and complexity, while Stacking Ensemble should be considered when\n",
    "   maximum accuracy is critical and training time is not a constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## Conclusion: The Art and Science of Machine Learning\n",
    "\n",
    "As we conclude our exploration of tree-based methods and ensemble learning, it's worth reflecting on the broader journey we've undertaken throughout this course. We began with fundamental statistical concepts and linear models, progressed through increasingly complex algorithms, and have now arrived at state-of-the-art ensemble techniques that represent some of the most powerful tools in the modern machine learning toolkit.\n",
    "\n",
    "The ensemble methods we've explored—bagging, boosting, voting, and stacking—demonstrate an important principle in machine learning: by combining multiple perspectives, we often achieve greater insight than any single model can provide alone. This mirrors the collaborative nature of scientific progress itself.\n",
    "\n",
    "However, as our final analysis of cost-performance tradeoffs reveals, machine learning is not merely a quest for the highest accuracy. The most sophisticated model is not always the most appropriate solution. The true art of applied machine learning lies in balancing theoretical performance with practical constraints—computational resources, interpretability needs, and deployment requirements.\n",
    "\n",
    "As you move forward in your careers, remember that the techniques covered in this course are not just algorithms to be implemented, but frameworks for thinking about problems. The process of feature selection, model development, rigorous validation, and thoughtful evaluation represents a systematic approach to knowledge discovery that extends far beyond prediction tasks.\n",
    "\n",
    "The field continues to evolve rapidly, but the fundamental principles we've covered—understanding your data, avoiding overfitting, properly validating models, and critically evaluating performance—will remain essential regardless of which new algorithms emerge. \n",
    "\n",
    "I encourage you to maintain both technical rigor and creative curiosity as you apply these tools to solve meaningful problems in your respective fields. The most impressive machine learning systems are not those with the most complex architectures, but those that most effectively address real-world needs.\n",
    "\n",
    "Thank you for your engagement and hard work throughout this course. I look forward to seeing how you will use these methods to extract knowledge from data and drive innovation in your future endeavors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
