{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multi-Label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook covers multi-label classification. It is based on Chapter 3 of HOML.\n",
    "\n",
    "We'll be exploring the MNIST dataset, a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. MNIST is considered the \"hello world\" of classification. It provides an opportunity for us to explore how ML is applied to image data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Yet another way to load a dataset... this time with `fetch_openml` from `sklearn.datasets`. This will take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# use as_frame=False to get data as NumPy arrays\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "type(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We've specified NumPy arrays as they are better suited to working with image data than DataFrames.\n",
    "\n",
    "The `Bunch` object is a kind of dictionary where the keys can be accessed as attributes. Most contain:\n",
    "\n",
    "- `DESCR`: a description of the dataset\n",
    "- `data`: the input data\n",
    "- `target`: the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Extract the data and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist.data, mnist.target\n",
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Image data is typically represented as a series of pixels, each with \"color\" data in red, green, blue values. For example, using 8-bit color where there are $2^8=256$ possible values, a red pixel is represented by $(255, 0, 0)$. In this case, the images are greyscale, so each pixel only has an intensity value in the $[0,255]$ range.\n",
    "\n",
    "From the results above, we can see that $X$ is an array of 70k lists, each with 784 elements, and that $y$ has the corresponding labels. To see what labels are present, we'll use NumPy's `unique` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We see that all digits from 0 to 9 are represented. It is important to know if the dataset is balanced, meaning that all classes are pretty equally represented. We can check that visually with a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of {value: count}\n",
    "unique_values, counts = np.unique(y, return_counts=True)\n",
    "digit_counts = dict(zip(unique_values, counts))\n",
    "\n",
    "# Plot as histogram using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(digit_counts.keys(), digit_counts.values())\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of digits in MNIST dataset')\n",
    "plt.xticks(list(digit_counts.keys()))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "This shows that the labels are reasonably well balanced. Data that appears perfectly balanced should be scrutinized as it is unusual in the real world.\n",
    "\n",
    "Each of the 784 elements associated with each list in $X$ represents a pixel. We can use Matplotlib's `imshow` function to look at an image, once the data is properly formatted. The images in this dataset are rectangular (2d), but the data is represented in a vector (1d), so we need to reshape it into the 28 x 28 array (28 x 28 = 784).\n",
    "\n",
    "This step is for our visualization only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a utility function to plot an image\n",
    "def plot_digit(image_data):\n",
    "    image = image_data.reshape(28, 28)\n",
    "    # use binary for greyscale images\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a digit and plot it\n",
    "some_digit = X[0]\n",
    "\n",
    "plot_digit(some_digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "I... guess that's a 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "It is!\n",
    "\n",
    "Before we go any further we need to follow best practices and set aside a test set. Per the dataset description, the first 60k images are for training and the last 10k are for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into the predefined train and test sets\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Training a Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We'll start by building a binary classifier that will discriminate between \"5\" and \"not-5\". We've done this before, but this is a good opportunity to explore the method in the context of a different kind of data. We'll also discuss a few additional nuances.\n",
    "\n",
    "Start by building a boolean mask for the training data based on the label \"5\". This will act as our labels for the binary classifier, indicating \"5\" (True) or \"not-5\" (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == '5')\n",
    "y_test_5 = (y_test == '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "SKL offers several optimization algorithms ('solvers') for logistic regression. While none use plain gradient descent, some solvers like `sag` and `saga` are based on that method. Regardless, they all benefit from standardizing features before fitting the model. This improves model convergence by preventing features with larger scales from dominating the optimization process. This preprocessing step is considered a best practice for logistic regression, even when using the default solver (`lbgfs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Only transform the test data (no fitting)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Now we'll fit the model as usual, and log the elapsed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_bin = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "logr_bin.fit(X_train_scaled, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Does it correctly predict an image we know is labeled \"5\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we do predictions we need to redefine `some_digit`,\n",
    "# which currently refers to X[0], unscaled data\n",
    "# since the model is fit using scaled data, we need to work with that\n",
    "# from now on!\n",
    "\n",
    "some_digit = X_train_scaled[0]\n",
    "\n",
    "logr_bin.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Yes! But how does it do overall? Let's perform train / test validation to get a gross approximation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "train_score = logr_bin.score(X_train_scaled, y_train_5)\n",
    "\n",
    "# Test accuracy (using the actual test set)\n",
    "test_score = logr_bin.score(X_test_scaled, y_test_5) \n",
    "# Note: You'll need to create y_test_5 with: y_test_5 = (y_test == '5')\n",
    "\n",
    "print(f\"Training accuracy: {train_score:.4f}\")\n",
    "print(f\"Test accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Both are over 97% - that seems very high! There are (at least) four questions that should come to mind based on this result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### What is accuracy, exactly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Accuracy is, as you might expect, the percentage of correct predictions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Accuracy} &= \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}} \\\\[5pt]\n",
    "&= \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Number of Observations}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is an imperfect measure of performance, as we'll soon see, and there are many other metrics to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Training accuracy is higher than test. What does that imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "When performance in training exceeds that in test, it may suggest some degree of overfitting.\n",
    "\n",
    "In this case, the difference is very small, suggesting that the model generalizes well. For overfitting, we would typically expect a larger gap; often on the order of a few percentage points.\n",
    "\n",
    "Keep in mind that this is a simple estimate of performance and that cross-validation would give a more rigorous result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### How good is 97%?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "97% accuracy sounds impressive at first glance, but we need context to truly evaluate it. Is it actually good, or just seemingly good? To answer this properly, we should establish a baseline model for comparison.\n",
    "\n",
    "For binary classification, it is common to use SKL's `DummyClassifier` as a baseline. This model simply predicts everything as the most commonly observed class in the dataset. For our 5 / not-five model, the most common label is `False` (not-five), since only about 10% of the images are for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_bin = DummyClassifier()\n",
    "dummy_bin.fit(X_train_scaled, y_train_5)\n",
    "\n",
    "# does it predict any True values?\n",
    "print(any(dummy_bin.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "This result indicates that no 5s are detected, which matches our expectations. Now let's score it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "train_score = dummy_bin.score(X_train_scaled, y_train_5)\n",
    "\n",
    "# Test accuracy (using the actual test set)\n",
    "test_score = dummy_bin.score(X_test_scaled, y_test_5) \n",
    "# Note: You'll need to create y_test_5 with: y_test_5 = (y_test == '5')\n",
    "\n",
    "print(f\"Training accuracy: {train_score:.4f}\")\n",
    "print(f\"Test accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Even a trivial model gets 90%+ accuracy for this data. That should also be expected since the dataset is balanced with 10 labels, so only 1/10th of our predictions should be wrong if we always predict not-5.\n",
    "\n",
    "This clearly demonstrates why accuracy is not the generally preferred performance measure for classifiers, especially when dealing with imbalanced (aka skewed) datasets. Instead we will rely on interpreting the confusion matrix in a comprehensive fashion, weighing the strengths and weaknesses of our model with the business objectives in mind.\n",
    "\n",
    "We will return to model evaluation shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## How can I understand the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "It depends on the questions you have. Here we will identify images that the model finds difficult to classify.\n",
    "\n",
    "First, we'll use the `decision_function` to identify images that are near the decision threshold. The `decision_function` returns the linear combination of features and weights, i.e.\n",
    "\n",
    "$$z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n$$\n",
    "\n",
    "This is the raw $z$ score before applying the sigmoid function,\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "The $z$ value does not return probabilities in the range $[0, 1]$. It ranges from negative infinity to positive infinity, with $0$ being the actual decision boundary. For a binary classifier, positive $z$ values mean the model predicts class $1$ (\"5\"), and negative values for the $0$ class (\"not-5\"). Zero is the decision boundary, so observations with a $z$ value near it are weakly classified.\n",
    "\n",
    "In the code below, the expression `np.argsort(np.abs(decision_scores))[:25]` returns the indicies that would sort the absolute values smallest to largest and selects the 25 examples closest to zero. Those results are plotted, with their predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the decision function values for all training examples\n",
    "decision_scores = logr_bin.decision_function(X_train_scaled)\n",
    "\n",
    "# Find examples near the decision boundary (small absolute scores)\n",
    "boundary_indices = np.argsort(np.abs(decision_scores))[:25]\n",
    "\n",
    "# Plot these examples\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, idx in enumerate(boundary_indices):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(X_train[idx].reshape(28, 28), cmap='binary')\n",
    "    plt.title(f\"{'5' if y_train_5[idx] else 'not-5'}\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Digits Near Decision Boundary')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Looking at the images, we can see why the model might struggle with these particular digits:\n",
    "\n",
    "- Some \"5\"s are written in unusual ways that make them look similar to other digits\n",
    "- Several \"3\"s appear that could be mistaken for \"5\"s due to similar curved structures\n",
    "- Some \"8\"s and \"6\"s near the decision boundary share structural similarities with \"5\"s\n",
    "- There are even some actual \"5\"s labeled as \"not-5\" and vice versa, suggesting possible labeling errors or extremely ambiguous handwriting\n",
    "\n",
    "The middle row shows how a curved \"6\" could be confused with a \"5\", while the true \"5\"s in row 4 have various styles that demonstrate the variability in handwriting the model must handle.\n",
    "\n",
    "This visualization reveals the complexity of the classification problem beyond abstract accuracy metrics, showing the actual edge cases where human interpretation might even disagree with the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## Training a Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "We'll demonstrate the use of OvR, OvO, and direct multinomial methods with Logistic Regression, as described in the slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Compare Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "First using the default multinomial method. `max_iter` was set to 1000 because the default value of 100 did not converge (you will get a warning message in this case). Alternatively (or additionally), you may find other solvers converge better in some cases.\n",
    "\n",
    "Note that this method leverages all CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_clf = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Using Jupyter's `%%timeit` magic method to measure the time required. This runs the cell in a different **namespace** so any variable created in it isn't available afterwards. Hence, I created the `multi_clf` instance first, before timing the `fit` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "multi_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "Since the `fit` method updates the existing `multi_clf` object directly (in-place modification), it is still accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on both training and test sets\n",
    "train_multi_score = multi_clf.score(X_train_scaled, y_train)\n",
    "test_multi_score = multi_clf.score(X_test_scaled, y_test)\n",
    "print(f\"Multinomial Training Accuracy: {train_multi_score:.4f}\")\n",
    "print(f\"Multinomial Test Accuracy: {test_multi_score:.4f}\")\n",
    "print(f\"Difference (Train-Test): {train_multi_score-test_multi_score:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Next we'll use OvR. This will train 10 classifiers - one for each digit - using 60k observations of 784 values. In spite of that, it takes only about 2x as long, thanks to efficient utilization of CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "ovr_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on both training and test sets\n",
    "train_ovr_score = ovr_clf.score(X_train_scaled, y_train)\n",
    "test_ovr_score = ovr_clf.score(X_test_scaled, y_test)\n",
    "print(f\"OvR Training Accuracy: {train_ovr_score:.4f}\")\n",
    "print(f\"OvR Test Accuracy: {test_ovr_score:.4f}\")\n",
    "print(f\"Difference (Train-Test): {train_ovr_score - test_ovr_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "Finally, we'll use OvO. This will train 10 * 9 / 2 = 45 different classifiers, each on a subset of the data. In this case it runs slightly faster than the OvR approach, despite many more models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovo_clf = OneVsOneClassifier(LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "ovo_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on both training and test sets\n",
    "train_ovo_score = ovo_clf.score(X_train_scaled, y_train)\n",
    "test_ovo_score = ovo_clf.score(X_test_scaled, y_test)\n",
    "print(f\"OvO Training Accuracy: {train_ovo_score:.4f}\")\n",
    "print(f\"OvO Test Accuracy: {test_ovo_score:.4f}\")\n",
    "print(f\"Difference (Train-Test): {train_ovo_score - test_ovo_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Summarize the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've already run the models and collected these scores\n",
    "# Store all results in a dictionary\n",
    "results = {\n",
    "    'Method': ['Multinomial', 'OvR', 'OvO'],\n",
    "    'Training Accuracy': [train_multi_score, train_ovr_score, train_ovo_score],\n",
    "    'Test Accuracy': [test_multi_score, test_ovr_score, test_ovo_score]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate the difference\n",
    "results_df['Difference (Train-Test)'] = results_df['Training Accuracy'] - results_df['Test Accuracy']\n",
    "\n",
    "# Display the table\n",
    "print(\"Comparison of Classification Methods:\")\n",
    "print(results_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for accuracies\n",
    "ax1 = plt.subplot(121)\n",
    "results_df_melt = pd.melt(results_df, \n",
    "                          id_vars=['Method'], \n",
    "                          value_vars=['Training Accuracy', 'Test Accuracy'],\n",
    "                          var_name='Dataset', value_name='Accuracy')\n",
    "sns.barplot(x='Method', y='Accuracy', hue='Dataset', data=results_df_melt, ax=ax1)\n",
    "ax1.set_title('Training vs Test Accuracy')\n",
    "ax1.set_ylim(0.85, 1.0)  # Adjust as needed to make differences visible\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Bar plot for differences\n",
    "ax2 = plt.subplot(122)\n",
    "sns.barplot(x='Method', y='Difference (Train-Test)', data=results_df, ax=ax2)\n",
    "ax2.set_title('Overfitting (Train-Test Difference)')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim(0, 0.05)  # Adjust as needed\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "At least in this case, the choice between these methods involves trade-offs:\n",
    "\n",
    "- For highest possible accuracy, OvO wins\n",
    "- For best generalization, OvR is most stable\n",
    "- Multinomial is a good middle ground\n",
    "\n",
    "These results align with theoretical expectations: OvO trains many specialized classifiers that can better capture complex decision boundaries but may learn some training set noise, while OvR's simpler approach generalizes more consistently but with slightly lower overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "As we did for binary classification, we'll create a confusion matrix to better see how this model is predicting each class.\n",
    "\n",
    "We'll use the `ConfusionMatrixDisplay` to easily create that based on model predictions for the test data. Here, `normalize` and `values_format` are used to display each cell as a percentage of total number of images in the row (the true class). For example, the bottom left cell tells us that 1% of images labeled \"9\" were misclassified as \"0\". Normalizing in this fashion allows us to make direct comparisons of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# For multiclass classifier (using OvR as an example)\n",
    "y_pred_multi = ovr_clf.predict(X_test_scaled)\n",
    "plt.figure(figsize=(10, 8))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_multi,\n",
    "                                       normalize=\"true\", \n",
    "                                       values_format=\".0%\")\n",
    "plt.title(\"Multiclass Classifier Confusion Matrix - All Predictions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "Only 86% of the images of 5s were correctly classified. Most commonly, 5s were misclassified as 3s (4% of the time), with 8s (3%) and 6s (2%) aslo prevalent.\n",
    "\n",
    "To make errors stand out more, we can put zero weight on the correct predictions. This amplifies error patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = (y_pred_multi != y_test)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_multi,\n",
    "                                        sample_weight=sample_weight,\n",
    "                                        normalize=\"true\",\n",
    "                                        values_format=\".0%\")\n",
    "plt.title(\"Multiclass Classifier Confusion Matrix - Errors Only\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "From this we can see that, for examples, 1s are most commonly confused for 8s, and 4s are most commonly confused for 9s.\n",
    "\n",
    "Tools like these can help us see where the model is doing well or poorly. This information can then be used to target improvements in preprocessing, parameter tuning, feature engineering, model selection, etc., potentially leading to a more robust classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### Explore Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "Let's look at the results of the multinomial approach. If we call the `decision_function` method for a single observation, it will return 10 scores, one for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_function expects an array of observations,\n",
    "# so we put it in a list\n",
    "z_values = multi_clf.decision_function([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of sorted z_values with their labels\n",
    "# decision_function returns an array of results,\n",
    "# one per observation, so we have to take the first [0]\n",
    "label_values = [(label, score) for label, score in enumerate(z_values[0])]\n",
    "\n",
    "sorted_pairs = sorted(label_values, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for label, score in sorted_pairs:\n",
    "    print(f\"{label}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "This shows that the multinomial model is most confident that the pixel data in `some_digit` is a $5$, with a $z$ value of 12.9053. Second most confident prediction is $3$, with 11.2705."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "In this case the result is rather trivial, since the indicies and class names match. In other circumstances it is more helpful to know the index of each label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "### Important Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "This analysis relies on a single predetermined train-test split without verifying that the observations are assigned to the splits in a random and balanced manner. This approach could introduce bias if the original data has inherent ordering. Cross-validation would provide more reliable performance estimates by reducing the variance in evaluation metrics and giving a better approximation of how the model would perform on unseen data. Additionally, no hyperparameter optimization was performed and important parameters like regularization strength (C), penalty types, or alternative solvers for LogisticRegression were not explored. These could significantly impact the relative performance and overfitting behaviors observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
