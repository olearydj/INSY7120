{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Classification - Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook discusses various strategies for dealing with imbalanced data. It also demonstrates probability calibration and threshold optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Install the `imbalanced-learn` package, which implements a number of over / under sampling methods in a SKL-friendly manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "First, create a synthetic dataset using SKL's `make_classification` function. This models fraudulent charges as the minority class (5%), with 5x higher amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset that mimics a fraud detection scenario\n",
    "def create_fraud_dataset(n_samples=10000, fraud_ratio=0.05, n_features=10):\n",
    "    \"\"\"\n",
    "    Create a synthetic fraud detection dataset with natural cost implications.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        The total number of samples.\n",
    "    fraud_ratio : float\n",
    "        The proportion of fraudulent transactions.\n",
    "    n_features : int\n",
    "        The number of features.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : DataFrame\n",
    "        The feature matrix.\n",
    "    y : Series\n",
    "        The target vector (0: legitimate, 1: fraudulent).\n",
    "    \"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=5,\n",
    "        n_redundant=3,\n",
    "        n_repeated=0,\n",
    "        n_classes=2,\n",
    "        weights=[1-fraud_ratio, fraud_ratio],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas for easier handling\n",
    "    feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "    X_df = pd.DataFrame(X, columns=feature_names)\n",
    "    y_df = pd.Series(y, name='fraud')\n",
    "    \n",
    "    # Add a transaction amount feature (higher for fraudulent transactions)\n",
    "    # This will help illustrate the cost implications\n",
    "    amounts = np.random.exponential(scale=100, size=n_samples)\n",
    "    # Make fraudulent transactions have higher amounts on average\n",
    "    amounts[y == 1] = amounts[y == 1] * 5\n",
    "    X_df['transaction_amount'] = amounts\n",
    "    \n",
    "    return X_df, y_df\n",
    "\n",
    "# Create the dataset\n",
    "X, y = create_fraud_dataset(n_samples=10000, fraud_ratio=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Display stats about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Number of samples: {len(X)}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n",
    "print(\"\\nClass distribution (%):\")\n",
    "print(y.value_counts(normalize=True) * 100)\n",
    "\n",
    "# Display transaction amount statistics by class\n",
    "fraud_amounts = X.loc[y == 1, 'transaction_amount']\n",
    "legit_amounts = X.loc[y == 0, 'transaction_amount']\n",
    "\n",
    "print(\"\\nTransaction Amount Statistics:\")\n",
    "print(f\"Legitimate transactions - Mean: ${legit_amounts.mean():.2f}, Median: ${legit_amounts.median():.2f}\")\n",
    "print(f\"Fraudulent transactions - Mean: ${fraud_amounts.mean():.2f}, Median: ${fraud_amounts.median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Look at the costs associated with fraudulent charges, assuming that the cost of a false positive (legitimate charge flagged as fraudulent) is, on average, $20 in customer service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate potential cost implications\n",
    "false_negative_cost = fraud_amounts.sum()\n",
    "false_positive_cost = legit_amounts.mean() * 20  # Assume customer service cost is $20 per false alert\n",
    "\n",
    "print(\"\\nCost Implications:\")\n",
    "print(f\"Total cost of missing all fraud (False Negatives): ${false_negative_cost:.2f}\")\n",
    "print(f\"Cost per false alert (False Positive): ${20:.2f}\")\n",
    "\n",
    "# Visualize the transaction amounts by class\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([legit_amounts, fraud_amounts], bins=50, alpha=0.6, \n",
    "         label=['Legitimate', 'Fraudulent'], color=['blue', 'red'])\n",
    "plt.title('Transaction Amount Distribution by Class')\n",
    "plt.xlabel('Transaction Amount ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize class imbalance\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x=y)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class (0: Legitimate, 1: Fraudulent)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add count labels\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', \n",
    "                (p.get_x() + p.get_width()/2., p.get_height()), \n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Train-test split with 70/30 and stratified sampling to ensure that the class balances are properly represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for later use\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(\"\\nTraining/Testing Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"Fraud ratio in training: {(y_train == 1).mean():.2%}\")\n",
    "print(f\"Fraud ratio in testing: {(y_test == 1).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Build and evaluate three baseline models:\n",
    "\n",
    "1. Majority class predictor\n",
    "2. Vanilla logistic regression \n",
    "3. Vanilla KNN\n",
    "\n",
    "But first, define a reusable function to evaluate and compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate and compare models\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model performance with various metrics\"\"\"\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "    print(f\"FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "For each model we will:\n",
    "1. instantiate the model\n",
    "2. fit the instance on training data\n",
    "3. generate predictions for the test data\n",
    "4. compare the predictions with the test labels to evaluate the model\n",
    "\n",
    "First use `DummyClassifier` to train a majority class predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_pred = dummy_clf.predict(X_test)\n",
    "dummy_metrics = evaluate_model(y_test, dummy_pred, \"Majority Class Predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Next, fit a `LogisticRegression` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_pred = logreg.predict(X_test)\n",
    "logreg_metrics = evaluate_model(y_test, logreg_pred, \"Vanilla Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Now, KNN with 5 neighbors using `KNeighborsClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_metrics = evaluate_model(y_test, knn_pred, \"Vanilla KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Compare the results in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "models = ['Majority Class', 'Logistic Regression', 'KNN']\n",
    "results = pd.DataFrame({\n",
    "    'Majority Class': dummy_metrics,\n",
    "    'Logistic Regression': logreg_metrics,\n",
    "    'KNN': knn_metrics\n",
    "}, index=metrics)\n",
    "\n",
    "# Display table\n",
    "print(\"\\n--- Model Comparison ---\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Based on F1-Score, LogReg comes out on top, so far. How does it fare on ROC AUC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the positive class\n",
    "y_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve points\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark some threshold points on the curve\n",
    "# Choose a few interesting thresholds to highlight\n",
    "threshold_indices = [\n",
    "   min(range(len(thresholds)), key=lambda i: abs(thresholds[i] - 0.3)),\n",
    "   min(range(len(thresholds)), key=lambda i: abs(thresholds[i] - 0.5)),\n",
    "   min(range(len(thresholds)), key=lambda i: abs(thresholds[i] - 0.7))\n",
    "]\n",
    "\n",
    "# Mark those points on the curve\n",
    "for i in threshold_indices:\n",
    "   plt.plot(fpr[i], tpr[i], 'ro')\n",
    "   plt.annotate(f'threshold = {thresholds[i]:.2f}', \n",
    "                (fpr[i], tpr[i]), \n",
    "                xytext=(10, -10),\n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print additional information\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nThreshold analysis:\")\n",
    "for i in threshold_indices:\n",
    "   print(f\"Threshold: {thresholds[i]:.2f}, FPR: {fpr[i]:.4f}, TPR: {tpr[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Class Weight Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, confusion_matrix, roc_auc_score)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate_model(y_true, y_pred, y_prob, model_name):\n",
    "   \"\"\"Evaluate model performance with various metrics\"\"\"\n",
    "   # Calculate metrics\n",
    "   accuracy = accuracy_score(y_true, y_pred)\n",
    "   precision = precision_score(y_true, y_pred)\n",
    "   recall = recall_score(y_true, y_pred)\n",
    "   f1 = f1_score(y_true, y_pred)\n",
    "   roc_auc = roc_auc_score(y_true, y_prob)\n",
    "   \n",
    "   # Print results\n",
    "   print(f\"\\n--- {model_name} ---\")\n",
    "   print(f\"Accuracy: {accuracy:.4f}\")\n",
    "   print(f\"Precision: {precision:.4f}\")\n",
    "   print(f\"Recall: {recall:.4f}\")\n",
    "   print(f\"F1 Score: {f1:.4f}\")\n",
    "   print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "   \n",
    "   # Display confusion matrix\n",
    "   cm = confusion_matrix(y_true, y_pred)\n",
    "   print(\"\\nConfusion Matrix:\")\n",
    "   print(f\"TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "   print(f\"FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "   \n",
    "   return accuracy, precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Now let's analyze the effect of class weight adjustment on the LogReg model. Not all models (e.g., KNN) support this approach.\n",
    "\n",
    "It is achieved with the `class_weight` argument, which can be set to several values. First, let's use the `balanced` option, which automatically adjusts weights inverely proportional to class frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_balanced = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "logreg_balanced.fit(X_train, y_train)\n",
    "logreg_balanced_pred = logreg_balanced.predict(X_test)\n",
    "logreg_balanced_prob = logreg_balanced.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Another option is to explicitly set the weights using a dictionary. Here we set fraud class as 20x ore important than non-fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 1, 1: 20}\n",
    "logreg_custom = LogisticRegression(class_weight=class_weights, solver='liblinear', random_state=42)\n",
    "logreg_custom.fit(X_train, y_train)\n",
    "logreg_custom_pred = logreg_custom.predict(X_test)\n",
    "logreg_custom_prob = logreg_custom.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Summarize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "print(\"=== CLASS WEIGHT ADJUSTMENT ANALYSIS ===\")\n",
    "\n",
    "# Baseline models (from previous cell)\n",
    "baseline_logreg_metrics = evaluate_model(y_test, logreg_pred, logreg.predict_proba(X_test)[:, 1], \"Baseline LogReg\")\n",
    "baseline_knn_metrics = evaluate_model(y_test, knn_pred, knn.predict_proba(X_test)[:, 1], \"Baseline KNN\")\n",
    "\n",
    "# Weighted models\n",
    "balanced_logreg_metrics = evaluate_model(y_test, logreg_balanced_pred, logreg_balanced_prob, \"Balanced LogReg\")\n",
    "custom_logreg_metrics = evaluate_model(y_test, logreg_custom_pred, logreg_custom_prob, \"Custom Weight LogReg\")\n",
    "\n",
    "# Compare all models in a table\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "results = pd.DataFrame({\n",
    "   'Baseline LogReg': baseline_logreg_metrics,\n",
    "   'Balanced LogReg': balanced_logreg_metrics,\n",
    "   'Custom LogReg': custom_logreg_metrics,\n",
    "   'Baseline KNN': baseline_knn_metrics\n",
    "}, index=metrics)\n",
    "\n",
    "# Display table\n",
    "print(\"\\n--- Model Comparison ---\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "From this we can see that the class weight adjustments improved the ROC AUC of our LogReg model, but the F1 Score decreased.\n",
    "\n",
    "Weighted models catch more of the minority class instances (higher recall) but generate more false alarms (lower precision). This effect increases with the weighting - custom recall > balanced, etc.\n",
    "\n",
    "Let's look at the ROC curves..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ROC curves for all models\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(y_true, y_prob, model_name, color):\n",
    "   fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "   roc_auc = roc_auc_score(y_true, y_prob)\n",
    "   plt.plot(fpr, tpr, color=color, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Plot curves for each model\n",
    "plot_roc_curve(y_test, logreg.predict_proba(X_test)[:, 1], 'Baseline LogReg', 'blue')\n",
    "plot_roc_curve(y_test, logreg_balanced_prob, 'Balanced LogReg', 'green')\n",
    "plot_roc_curve(y_test, logreg_custom_prob, 'Custom LogReg', 'red')\n",
    "plot_roc_curve(y_test, knn.predict_proba(X_test)[:, 1], 'Baseline KNN', 'purple')\n",
    "\n",
    "# Add diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Class Weight Adjustment Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Doesn't seem like much of a difference. What does a 0.0037 increase in ROC AUC (balanced - baseline) equate to in dollars? The following code calculates the associated costs and benefits for the 3000 observations in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the financial benefit of the Balanced LogReg model over Baseline LogReg\n",
    "\n",
    "# Get predictions from both models\n",
    "baseline_pred = logreg.predict(X_test)\n",
    "baseline_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "balanced_pred = logreg_balanced.predict(X_test)\n",
    "balanced_prob = logreg_balanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Get transaction amounts from test set\n",
    "test_amounts = X_test['transaction_amount'].values\n",
    "\n",
    "# Define a function to calculate financial impact\n",
    "def calculate_financial_impact(y_true, y_pred, transaction_amounts):\n",
    "    \"\"\"\n",
    "    Calculate financial impact of model predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True labels (0 = legitimate, 1 = fraud)\n",
    "    - y_pred: Predicted labels\n",
    "    - transaction_amounts: Transaction amounts in dollars\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with financial metrics\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    transaction_amounts = np.array(transaction_amounts)\n",
    "    \n",
    "    # Identify different prediction types\n",
    "    true_positives = (y_true == 1) & (y_pred == 1)  # Correctly identified fraud\n",
    "    false_negatives = (y_true == 1) & (y_pred == 0)  # Missed fraud\n",
    "    false_positives = (y_true == 0) & (y_pred == 1)  # False alarms\n",
    "    \n",
    "    # Calculate financial impact\n",
    "    saved_amount = np.sum(transaction_amounts[true_positives])  # Money saved by catching fraud\n",
    "    lost_amount = np.sum(transaction_amounts[false_negatives])  # Money lost by missing fraud\n",
    "    investigation_cost = np.sum(false_positives) * 20  # Cost of investigating false alarms ($20 per case)\n",
    "    \n",
    "    # Net financial impact\n",
    "    net_impact = saved_amount - investigation_cost\n",
    "    \n",
    "    return {\n",
    "        'Saved Amount': saved_amount,\n",
    "        'Lost Amount': lost_amount,\n",
    "        'Investigation Cost': investigation_cost,\n",
    "        'Net Financial Impact': net_impact,\n",
    "        'Total Fraud Amount': saved_amount + lost_amount,\n",
    "        'Caught Fraud Percentage': saved_amount / (saved_amount + lost_amount) * 100 if (saved_amount + lost_amount) > 0 else 0\n",
    "    }\n",
    "\n",
    "# Calculate financial impact for both models\n",
    "baseline_impact = calculate_financial_impact(y_test, baseline_pred, test_amounts)\n",
    "balanced_impact = calculate_financial_impact(y_test, balanced_pred, test_amounts)\n",
    "\n",
    "# Display results\n",
    "print(\"=== Financial Impact Analysis ===\")\n",
    "print(\"\\nBaseline LogReg Model:\")\n",
    "for metric, value in baseline_impact.items():\n",
    "    if 'Percentage' in metric:\n",
    "        print(f\"{metric}: {value:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{metric}: ${value:.2f}\")\n",
    "\n",
    "print(\"\\nBalanced LogReg Model:\")\n",
    "for metric, value in balanced_impact.items():\n",
    "    if 'Percentage' in metric:\n",
    "        print(f\"{metric}: {value:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{metric}: ${value:.2f}\")\n",
    "\n",
    "# Calculate difference\n",
    "difference = {key: balanced_impact[key] - baseline_impact[key] for key in baseline_impact}\n",
    "\n",
    "print(\"\\nDifference (Balanced - Baseline):\")\n",
    "for metric, value in difference.items():\n",
    "    if 'Percentage' in metric:\n",
    "        print(f\"{metric}: {value:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{metric}: ${value:.2f}\")\n",
    "\n",
    "# Calculate ROI\n",
    "if difference['Investigation Cost'] > 0:\n",
    "    roi = (difference['Saved Amount'] - difference['Investigation Cost']) / difference['Investigation Cost'] * 100\n",
    "    print(f\"\\nROI: {roi:.2f}%\")\n",
    "else:\n",
    "    print(\"\\nROI: Cannot calculate (no additional investigation cost)\")\n",
    "\n",
    "# Plot the financial comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['Saved Amount', 'Lost Amount', 'Investigation Cost', 'Net Financial Impact']\n",
    "baseline_values = [baseline_impact[m] for m in metrics]\n",
    "balanced_values = [balanced_impact[m] for m in metrics]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "rects1 = ax.bar(x - width/2, baseline_values, width, label='Baseline LogReg')\n",
    "rects2 = ax.bar(x + width/2, balanced_values, width, label='Balanced LogReg')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_title('Financial Impact Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'${height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', rotation=45)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a second plot showing percentage of caught fraud\n",
    "labels = ['Baseline LogReg', 'Balanced LogReg']\n",
    "caught_percentages = [baseline_impact['Caught Fraud Percentage'], balanced_impact['Caught Fraud Percentage']]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, caught_percentages, color=['blue', 'green'])\n",
    "plt.title('Percentage of Fraud Amount Caught by Model')\n",
    "plt.ylabel('Percentage of Total Fraud Amount')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add percentage labels above bars\n",
    "for i, v in enumerate(caught_percentages):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "The balanced model saved about \\\\$10k (net financial impact difference) by catching 20% more fraud than the baseline. That is over only 3k observations. Visa alone processed 720m transactions per **day** in 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Over / Under Sampling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "We'll use over and under sampling techniques with the KNN model, which didn't support class weight adjustment.\n",
    "\n",
    "First, refit and evaluate the baseline KNN again to remind us of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Baseline KNN model (from previous cells)\n",
    "print(\"=== BASELINE KNN MODEL ===\")\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_prob = knn.predict_proba(X_test)[:, 1]\n",
    "baseline_metrics = evaluate_model(y_test, knn_pred, knn_prob, \"Baseline KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Next, use SMOTE (Synthetic Minority Over-sampling Technique), which generates synthetic examples of the minority class (fraud) by interpolating between existing minority class examples.\n",
    "\n",
    "By adding new observations based on the existing data (not copies of observations), it creates a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SMOTE Oversampling\n",
    "print(\"\\n=== SMOTE OVERSAMPLING ===\")\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display class distribution after SMOTE\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Note that the size of the training data has nearly doubled. This can introduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_smote.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Use the SMOTE'd data to fit a new KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN on SMOTE-resampled data\n",
    "knn_smote = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_smote.fit(X_train_smote, y_train_smote)\n",
    "knn_smote_pred = knn_smote.predict(X_test)\n",
    "knn_smote_prob = knn_smote.predict_proba(X_test)[:, 1]\n",
    "smote_metrics = evaluate_model(y_test, knn_smote_pred, knn_smote_prob, \"KNN with SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Again, ROC AUC has increased but F1 has decreased, as gains in Precision take away from Recall, or vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Next, let's try random undersampling, where random examples from the majority class are simply removed from the dataset. Not surprisingly, this can result in information loss. Yes, it is a real approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Random Undersampling\n",
    "print(\"\\n=== RANDOM UNDERSAMPLING ===\")\n",
    "# Apply random undersampling to the training data\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display class distribution after undersampling\n",
    "print(\"\\nClass distribution after undersampling:\")\n",
    "print(pd.Series(y_train_rus).value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "We're now working with 1/10th the original data!\n",
    "\n",
    "Fit a new model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN on undersampled data\n",
    "knn_rus = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_rus.fit(X_train_rus, y_train_rus)\n",
    "knn_rus_pred = knn_rus.predict(X_test)\n",
    "knn_rus_prob = knn_rus.predict_proba(X_test)[:, 1]\n",
    "rus_metrics = evaluate_model(y_test, knn_rus_pred, knn_rus_prob, \"KNN with Random Undersampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Baseline KNN', 'KNN with SMOTE', 'KNN with Undersampling']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "results = pd.DataFrame({\n",
    "    models[0]: baseline_metrics,\n",
    "    models[1]: smote_metrics,\n",
    "    models[2]: rus_metrics\n",
    "}, index=metrics)\n",
    "\n",
    "print(\"\\n--- Model Comparison ---\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "The baseline model is very cautious about flagging transactions as fraudulent (high precision, low recall). The resampled models are more aggressive, catching more fraud (higher recall) but generating more false alarms (lower precision).\n",
    "\n",
    "Both resampling techniques show meaningful improvements in ROC AUC. This is significant because ROC AUC measures performance across all possible thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "## Probability Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "We'll demonstrate this using the LogReg model we built with custom weights:\n",
    "\n",
    "```text\n",
    "                Custom LogReg\n",
    "Accuracy             0.853667\n",
    "Precision            0.247253\n",
    "Recall               0.828221\n",
    "F1 Score             0.380818\n",
    "ROC AUC              0.923279\n",
    "```\n",
    "\n",
    "It is a good candidate because:\n",
    "- It shows the highest ROC AUC (0.923279), indicating excellent discriminative ability\n",
    "- It has the most severe precision-recall trade-off (very low precision of 0.247 with very high recall of 0.828)\n",
    "- Class weighting directly influences the model's probability estimates by modifying the loss function\n",
    "- The dramatic gap between accuracy (0.854) and ROC AUC (0.923) suggests probabilities are poorly calibrated with the default threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "First we'll generate a reliability diagram (calibration curve) for the custom LogReg model, to illustrate how far out of calibration its probs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a reliability diagram (calibration curve) for the custom LogReg model\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Get predicted probabilities from the custom weighted model\n",
    "custom_probs = logreg_custom.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, custom_probs, n_bins=10)\n",
    "\n",
    "# Create reliability diagram\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Plot perfectly calibrated line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated', color='gray')\n",
    "\n",
    "# Plot calibration curve\n",
    "plt.plot(prob_pred, prob_true, marker='o', linewidth=2, \n",
    "        label=f'Custom LogReg (Brier score: {brier_score_loss(y_test, custom_probs):.3f})',\n",
    "        color='red')\n",
    "\n",
    "# Add histogram of predicted probabilities\n",
    "plt.hist(custom_probs, bins=10, range=(0, 1), alpha=0.2, color='red', \n",
    "        density=True, align='mid', label='Histogram')\n",
    "\n",
    "# Add details\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives (Actual Frequency)')\n",
    "plt.title('Calibration Curve - Custom Weighted Logistic Regression')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate key points where calibration is poor\n",
    "max_deviation_idx = np.argmax(np.abs(prob_true - prob_pred))\n",
    "plt.annotate(f'Poor calibration\\n({prob_pred[max_deviation_idx]:.2f}, {prob_true[max_deviation_idx]:.2f})',\n",
    "            xy=(prob_pred[max_deviation_idx], prob_true[max_deviation_idx]),\n",
    "            xytext=(prob_pred[max_deviation_idx] + 0.1, prob_true[max_deviation_idx] - 0.1),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n",
    "            fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "In this plot, the X axis is the predicted probabilities and the y is the actual rate. For example, where the model predicts 84% chance of fraud, it occurs only 24% of the time. The model dramatically underestimates true probabilities.\n",
    "\n",
    "Points above the diagonal show where the model overestimates probabilities. Points below the diagonal are underestimated. This model dramatically overestimates probabilities in order to improve the likelihood of correctly identifying fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "How Platt Scaling Works:\n",
    "1. Platt scaling fits a logistic regression model to the original model's predictions\n",
    "2. This creates a mapping function from original scores to calibrated probabilities\n",
    "3. The mapping preserves the ranking of predictions (same ROC AUC) but adjusts the scale\n",
    "4. The calibrated probabilities more accurately reflect the true likelihood of fraud\n",
    "5. This enables better decision-making when selecting probability thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply probability calibration to improve the Custom LogReg model\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import brier_score_loss, precision_recall_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's split our training data to have a separate calibration set\n",
    "# This avoids calibrating on the same data used for model training\n",
    "X_train_model, X_train_calib, y_train_model, y_train_calib = train_test_split(\n",
    "   X_train, y_train, test_size=0.3, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Train base model on the training subset\n",
    "base_model = LogisticRegression(class_weight={0:1, 1:20}, solver='liblinear', random_state=42)\n",
    "base_model.fit(X_train_model, y_train_model)\n",
    "\n",
    "# Create calibrated model using Platt scaling (sigmoid method)\n",
    "calibrated_model = CalibratedClassifierCV(base_model, method='sigmoid', cv='prefit')\n",
    "calibrated_model.fit(X_train_calib, y_train_calib)\n",
    "\n",
    "# Get predictions from both models on test set\n",
    "base_probs = base_model.predict_proba(X_test)[:, 1]\n",
    "calibrated_probs = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate calibration curves for both models\n",
    "prob_true_base, prob_pred_base = calibration_curve(y_test, base_probs, n_bins=10)\n",
    "prob_true_calib, prob_pred_calib = calibration_curve(y_test, calibrated_probs, n_bins=10)\n",
    "\n",
    "# Calculate Brier scores (lower is better)\n",
    "brier_base = brier_score_loss(y_test, base_probs)\n",
    "brier_calib = brier_score_loss(y_test, calibrated_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reliability diagram comparing the two models\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Plot perfectly calibrated line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated', color='gray')\n",
    "\n",
    "# Plot calibration curves\n",
    "plt.plot(prob_pred_base, prob_true_base, marker='o', linewidth=2, \n",
    "        label=f'Original Custom LogReg (Brier: {brier_base:.3f})',\n",
    "        color='red')\n",
    "plt.plot(prob_pred_calib, prob_true_calib, marker='s', linewidth=2, \n",
    "        label=f'Platt Calibrated LogReg (Brier: {brier_calib:.3f})',\n",
    "        color='blue')\n",
    "\n",
    "# Add histograms of predicted probabilities\n",
    "plt.hist(base_probs, bins=10, range=(0, 1), alpha=0.2, color='red', \n",
    "        density=True, align='mid')\n",
    "plt.hist(calibrated_probs, bins=10, range=(0, 1), alpha=0.2, color='blue', \n",
    "        density=True, align='mid')\n",
    "\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives (Actual Frequency)')\n",
    "plt.title('Reliability Diagram - Before and After Platt Scaling')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add explanation\n",
    "plt.figtext(0.15, 0.02, \n",
    "           \"Platt scaling fits a logistic regression to model outputs\\n\" +\n",
    "           \"to map them to calibrated probabilities\", \n",
    "           fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics for both models\n",
    "print(\"Calibration Comparison:\")\n",
    "print(f\"Original model Brier score: {brier_base:.4f}\")\n",
    "print(f\"Calibrated model Brier score: {brier_calib:.4f}\")\n",
    "print(f\"Improvement in Brier score: {(1 - brier_calib/brier_base)*100:.2f}%\")\n",
    "\n",
    "# Calculate mean absolute calibration error\n",
    "calib_error_base = np.mean(np.abs(prob_true_base - prob_pred_base))\n",
    "calib_error_calib = np.mean(np.abs(prob_true_calib - prob_pred_calib))\n",
    "print(f\"Original model mean absolute calibration error: {calib_error_base:.4f}\")\n",
    "print(f\"Calibrated model mean absolute calibration error: {calib_error_calib:.4f}\")\n",
    "print(f\"Improvement in calibration error: {(1 - calib_error_calib/calib_error_base)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "## Threshold Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "Demonstrate this with the calibrated LogReg model:\n",
    "\n",
    "- highest ROC AUC (0.923279) implies best performance over all thresholds\n",
    "- already calibrated!\n",
    "\n",
    "First, define a function that calculates the cost for a given threshold, true classes, predicted probabilities, and the FP cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_cost(y_true, y_prob, threshold, fp_cost=20):\n",
    "    \"\"\"\n",
    "    Calculate the expected cost given probabilities and a threshold\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True labels (0=legitimate, 1=fraud)\n",
    "    - y_prob: Predicted probabilities of fraud\n",
    "    - threshold: Probability threshold for classification\n",
    "    - fp_cost: Cost of investigating a false positive ($)\n",
    "    \n",
    "    Returns:\n",
    "    - Total expected cost\n",
    "    \"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Get transaction amounts from test set\n",
    "    transaction_amounts = X_test['transaction_amount'].values\n",
    "    \n",
    "    # Identify prediction types\n",
    "    false_negatives = (y_true == 1) & (y_pred == 0)  # Missed fraud\n",
    "    false_positives = (y_true == 0) & (y_pred == 1)  # False alarms\n",
    "    \n",
    "    # Calculate costs\n",
    "    fn_cost = np.sum(transaction_amounts[false_negatives])  # Lost money\n",
    "    fp_cost_total = np.sum(false_positives) * fp_cost  # Investigation cost\n",
    "    \n",
    "    return fn_cost + fp_cost_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "Use that function to calculate the cost for a range of threshold values and find the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cost-optimal threshold by trying a range of thresholds\n",
    "thresholds_to_try = np.linspace(0.01, 0.99, 99)\n",
    "costs = []\n",
    "\n",
    "for threshold in thresholds_to_try:\n",
    "    cost = calculate_expected_cost(y_test, calibrated_probs, threshold)\n",
    "    costs.append(cost)\n",
    "\n",
    "# Find minimum cost threshold\n",
    "cost_optimal_idx = np.argmin(costs)\n",
    "cost_optimal_threshold = thresholds_to_try[cost_optimal_idx]\n",
    "min_cost = costs[cost_optimal_idx]\n",
    "\n",
    "print(f\"\\nCost-optimal threshold: {cost_optimal_threshold:.4f}\")\n",
    "print(f\"Minimum expected cost: ${min_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "Plot the cost vs threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds_to_try, costs, lw=2, color='blue')\n",
    "plt.axvline(x=cost_optimal_threshold, color='red', linestyle='--', \n",
    "           label=f'Optimal threshold: {cost_optimal_threshold:.3f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Expected Cost ($)')\n",
    "plt.title('Expected Cost vs. Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- At near-zero thresholds costs spike due to excessive false positives (investigation costs)\n",
    "- At near-one thresholds costs spike due to excessive false negatives (missed fraud)\n",
    "- In the \"saddle\" the FP / FN tradeoff plays out at a rate proportional to the cost differences\n",
    "\n",
    "Left of the optimal point the cost of FPs dominate. Right of it, the costs are dominated by FNs. Moving left from the optimal increases FPs faster than it reduces FNs. Moving right the opposite is true.\n",
    "\n",
    "This is a very valuable, actionable chart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "Create a summary of financial impact for different threshold values.\n",
    "\n",
    "First, define a helper function to calculate the values of interest.\n",
    "\n",
    "- Saved = total cost of fraudulent charges avoided\n",
    "- Lost = total fraudulent charges missed\n",
    "- FP Cost = total cost of false postives (investigations)\n",
    "- Net Impact = saved - fp cost\n",
    "- Alerts = TP + FP\n",
    "- Fraud Cases Caught = TP\n",
    "- Fraud Cases Missed = FN\n",
    "\n",
    "**Why doesn't Lost factor into Net Impact?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_financial_metrics(y_true, y_prob, threshold, fp_cost=20):\n",
    "    \"\"\"Calculate comprehensive financial metrics for a given threshold\"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Get confusion matrix elements\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Get transaction amounts from test set\n",
    "    transaction_amounts = X_test['transaction_amount'].values\n",
    "    \n",
    "    # Identify different prediction types\n",
    "    true_positives = (y_true == 1) & (y_pred == 1)  # Correctly identified fraud\n",
    "    false_negatives = (y_true == 1) & (y_pred == 0)  # Missed fraud\n",
    "    \n",
    "    # Calculate financial impact\n",
    "    saved_amount = np.sum(transaction_amounts[true_positives])\n",
    "    lost_amount = np.sum(transaction_amounts[false_negatives])\n",
    "    investigation_cost = fp * fp_cost\n",
    "    \n",
    "    # Net financial impact\n",
    "    net_impact = saved_amount - investigation_cost\n",
    "    \n",
    "    return {\n",
    "        'Thresh': threshold,\n",
    "        'Saved': saved_amount,\n",
    "        'Lost': lost_amount,\n",
    "        'FP Cost': investigation_cost,\n",
    "        'Net Impact': net_impact,\n",
    "        'Prec': precision_score(y_true, y_pred) if tp + fp > 0 else 0,\n",
    "        'Recall': recall_score(y_true, y_pred) if tp + fn > 0 else 0,\n",
    "        'FPR': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        'Alerts': tp + fp,\n",
    "        'Fraud Cases Caught': tp,\n",
    "        'Fraud Cases Missed': fn\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "Calculate metrics at thresholds of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_to_compare = {\n",
    "    'Default (0.5)': 0.5,\n",
    "    'Cost-Optimal': cost_optimal_threshold,\n",
    "    'High-Recall (0.05)': 0.05,\n",
    "    'High-Precision (0.95)': 0.95\n",
    "}\n",
    "\n",
    "# Calculate metrics for each threshold\n",
    "financial_results = []\n",
    "for name, threshold in thresholds_to_compare.items():\n",
    "    metrics = calculate_financial_metrics(y_test, calibrated_probs, threshold)\n",
    "    metrics['Name'] = name\n",
    "    financial_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame for better display\n",
    "results_df = pd.DataFrame(financial_results)\n",
    "results_df = results_df[['Name', 'Thresh', 'Net Impact', \n",
    "                        'Saved', 'Lost', 'FP Cost',\n",
    "                        'Prec', 'Recall', \n",
    "                        'FPR', 'Alerts']]\n",
    "\n",
    "# Format as currency where appropriate\n",
    "for col in ['Net Impact', 'Saved', 'Lost', 'FP Cost']:\n",
    "    results_df[col] = results_df[col].map('${:,.0f}'.format)\n",
    "\n",
    "# Format percentages\n",
    "for col in ['Prec', 'Recall', 'FPR']:\n",
    "    results_df[col] = results_df[col].map('{:.1%}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Display results dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinancial Impact at Different Thresholds:\\n\")\n",
    "# print(results_df.to_string(index=False))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- Cost-optimal maximizes net impact by catching 71.8% of fraud while keeping investigation costs in hand\n",
    "- Default isn't very sensitive - it misses fraud (37.4%) but also has few FPs as a result\n",
    "- High-precision perfectly minimizes FPs\n",
    "- High-recall catches more fraud (82.8%) than cost-optimal, at the expense of higher FPR (14.3%) and associated costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
