{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook introduces decision trees.\n",
    "\n",
    "The Iris dataset is one of the most famous and widely used datasets in machine learning. It was introduced by the statistician Ronald Fisher in 1936 in his paper \"The Use of Multiple Measurements in Taxonomic Problems.\"\n",
    "\n",
    "The dataset contains 150 samples from three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor), with 50 samples from each species. For each flower, four features were measured (all in centimeters):\n",
    "\n",
    "- sepal length\n",
    "- sepal width\n",
    "- petal length\n",
    "- petal width\n",
    "\n",
    "The Iris dataset is often used for classification tasks because it provides a simple but non-trivial challenge. One class (Iris setosa) is linearly separable from the other two, while the other two classes (Iris virginica and Iris versicolor) have some overlap, making it a good test case for different classification algorithms. It's commonly used for teaching machine learning concepts, demonstrating visualization techniques, and benchmarking classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train a decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with multiple plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Decision boundaries\n",
    "plt.subplot(2, 2, 1)\n",
    "h = 0.02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = tree_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.RdYlBu, edgecolor='k')\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Decision Boundaries')\n",
    "\n",
    "# Plot 2: Decision Tree Visualization\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_tree(tree_clf, filled=True, feature_names=iris.feature_names[:2], \n",
    "          class_names=iris.target_names, rounded=True)\n",
    "plt.title('Decision Tree')\n",
    "\n",
    "# Plot 3: Feature Importance\n",
    "plt.subplot(2, 2, 3)\n",
    "importance = tree_clf.feature_importances_\n",
    "plt.bar(iris.feature_names[:2], importance)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "# Plot 4: Confusion Matrix\n",
    "plt.subplot(2, 2, 4)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some information about the tree\n",
    "print(f\"Number of nodes: {tree_clf.tree_.node_count}\")\n",
    "print(f\"Tree depth: {tree_clf.get_depth()}\")\n",
    "print(f\"Number of leaves: {tree_clf.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The plots provide a comprehensive view of how the decision tree is classifying the Iris flowers:\n",
    "\n",
    "1. **Decision Boundaries**: The plot shows how the decision tree divides the feature space using sepal length and width. The red, blue, and yellow regions represent the three Iris species. The decision boundaries are perpendicular to the axes (characteristic of decision trees), creating rectangular regions. There's a clear vertical boundary around sepal length = 5, which appears to separate one species (likely Setosa) from the others.\n",
    "\n",
    "2. **Decision Tree Structure**: The tree has a depth of 3 with 15 nodes total and 8 leaf nodes. The first split is on sepal length, which aligns with what we see in the decision boundaries. The tree's structure reveals the hierarchical decision-making process.\n",
    "\n",
    "3. **Feature Importance**: Sepal length is shown to be significantly more important (about 0.72) than sepal width (about 0.28). This explains why the first split in the tree is based on sepal length.\n",
    "\n",
    "4. **Confusion Matrix**: The model performs very well, particularly for setosa (18/19 correct). There's some confusion between versicolor and virginica, which is expected as these two species are known to have overlapping characteristics. The overall accuracy appears to be around (18+8+8)/45 â‰ˆ 76%, which is reasonable given we're only using two of the four available features.\n",
    "\n",
    "This visualization effectively demonstrates how decision trees make classification decisions with orthogonal boundaries. It also shows why we often need ensemble methods like random forests for better performance - a single decision tree has limitations in capturing the relationship between versicolor and virginica when using only sepal measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
