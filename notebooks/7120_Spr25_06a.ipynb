{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Load the packages and configure environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Using the Advertising data from ISL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data set directly from the web using pandas\n",
    "url = \"https://raw.githubusercontent.com/olearydj/INSY7120/refs/heads/main/notebooks/data/Advertising.csv\"\n",
    "sales = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic structure\n",
    "print(sales.head())\n",
    "print(\"Dataset shape:\", sales.shape)\n",
    "print(\"\\nData types:\\n\", sales.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\\n\", sales.isnull().sum())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nSummary statistics:\\n\")\n",
    "sales.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "What is the first column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall: the columns attribute of a DataFrame gives a list of column names\n",
    "col_name = sales.columns[0]\n",
    "\n",
    "# drop the column by name\n",
    "# by default drop refers to rows (axis=0), must specify cols\n",
    "sales.drop(col_name, axis=1, inplace=True)\n",
    "\n",
    "# inplace=True modifies sales in place; alternatively reassign the modified object\n",
    "# sales = sales.drop(col_name, axis=1)\n",
    "\n",
    "sales.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Caution: the previous cell is destructive. It relies only on positional information to delete columns. If you run it multiple times, it will delete predictors!\n",
    "\n",
    "So we have 200 rows of advertising spending categories (predictors) and associated sales (response) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Use the same process as before, condensed into a single section.\n",
    "\n",
    "**Step 1** - split the data into features (`X`) and target (`y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sales[['TV', 'radio', 'newspaper']]\n",
    "y = sales['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**Steps 2 and 3** - initialize and fit the model, review results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "mlr = LinearRegression()\n",
    "mlr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the estimated model parameters\n",
    "print(f\"Model Coefficients: {mlr.coef_}\")\n",
    "print(f\"Model Intercept: {mlr.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The MLR takes the form:\n",
    "\n",
    "$$y =  \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times radio + \\beta_3 \\times newspaper + \\epsilon$$\n",
    "\n",
    "Which we estimate (because $\\epsilon$ is unaccounted for) as:\n",
    "\n",
    "$$\\hat{y} =  2.939 + 0.046 \\times TV + 0.189 \\times radio - 0.001 \\times newspaper$$\n",
    "\n",
    "where the coefficients and thus the response are estimates:\n",
    "\n",
    "$$\\hat{\\beta}_0 = 2.939, \\hat{\\beta}_1 = 0.046, etc.$$\n",
    "\n",
    "What does this tell us about the relative importance of the features? Can we compare them directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "**Step 4** - evaluate model performance\n",
    "\n",
    "We'll use $R^2$ and $RMSE$, both of which are measures of the prediction error ($RSS$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import r2 and mse functions\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# generate predictions\n",
    "y_pred = mlr.predict(X)\n",
    "\n",
    "# use predictions to score\n",
    "r2 = r2_score(y, y_pred)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "In this case, what are we assessing the performance of? What are we generating predictions for? What do these results tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "**step 5** - generate predictions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.array([[0, 0, 180],    # all newspaper\n",
    "                     [0, 180, 0],    # all radio\n",
    "                     [180, 0, 0],    # all TV\n",
    "                     [60, 60, 60]])  # equal mix\n",
    "\n",
    "new_data = pd.DataFrame(new_data, columns=['TV', 'radio', 'newspaper'])\n",
    "predicted_sales = mlr.predict(new_data)\n",
    "\n",
    "print(predicted_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "All newspaper gives the worst results. All radio is the best of these options, with all TV performing worse than a mixture. As you might expect, an equal mix is somewhere in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "**step 6** - interpret results (added)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "This suggests that our model explains about 90% of the variance in sales, with radio having the most influence over sales, followed by tv. Newspaper has a slightly negative impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "But there are several critical limitations:\n",
    "\n",
    "- fitted and evaluated the same data (training performance metrics)\n",
    "- didn't confirm linearity of predictors or investigate interactions between them\n",
    "- no inferential statistics\n",
    "- no domain expertise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Look at relationships between predictors and outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a figure with 3 subplots in one row\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Radio plot\n",
    "sns.regplot(data=sales, x='radio', y='sales', ax=ax1)\n",
    "ax1.set_title('Radio vs Sales')\n",
    "\n",
    "# TV plot \n",
    "sns.regplot(data=sales, x='TV', y='sales', ax=ax2)\n",
    "ax2.set_title('TV vs Sales')\n",
    "\n",
    "# Newspaper plot\n",
    "sns.regplot(data=sales, x='newspaper', y='sales', ax=ax3)\n",
    "ax3.set_title('Newspaper vs Sales')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Positive coefficient for all predictors. How do these results differ from the MLR estimates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between all variables\n",
    "correlations = sales[['TV', 'radio', 'newspaper', 'sales']].corr()\n",
    "\n",
    "# Round to 3 decimal places\n",
    "correlations_rounded = correlations.round(3)\n",
    "\n",
    "print(correlations_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Note the correlation between radio and newspaper (0.35). This indicates that markets with high newspaper advertising also tend to have high radio advertising.\n",
    "\n",
    "In the SLR setting we find that sales increases with newspaper spend, but MLR shows negligible effect. Newspaper advertising as a surrogate for radio advertising - the SLR chart above is really showing us the effect of increased radio spend that comes with increased newspaper.\n",
    "\n",
    "To understand this better we must turn to inferential methods, for which we use `statsmodels`.\n",
    "\n",
    "**Note:** the split focus of scikit-learn (prediction) and statsmodels (inference) is further evidence of the difference in focus between ML and Statistical Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant (intercept) to the predictors\n",
    "X_sm = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "mlr_sm = sm.OLS(y, X_sm).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "The results of this are summarized in three tables. We are primarily interested in the first two: overall regression results and feature-level details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall regression results\n",
    "mlr_sm.summary().tables[0]  # regression results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Most importantly, this tells us that the overall model is significant, with a p-value of near zero (1.58e-96). Specifically, the null hypothesis of the F-test is that all coefficients are zero. We reject this in favor of the alternative, which suggests that at least one coefficient is non-zero. In other words, the model explains significantly more variance than one with no predictors (just the intercept - a horizontal line), so at least one predictor has a real relationship with sales. The $R^2$ value equals the fit obtained with scikit-learn.\n",
    "\n",
    "The second table gives feature-level details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients and significance\n",
    "mlr_sm.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Again, the coefficients match those found by SKL. In addition, this table tells us a lot about the uncertainty associated with the predictions.\n",
    "\n",
    "Of particular interest, it shows that newspaper is not significant.\n",
    "\n",
    "Both results (coefficient and p-value) suggest simplifying the model by removing the newspaper predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = sales[['TV', 'radio']]\n",
    "y2 = sales['sales']\n",
    "\n",
    "mlr2 = LinearRegression()\n",
    "mlr2.fit(X2, y2)\n",
    "\n",
    "print(f\"Model Coefficients: {mlr2.coef_}\")\n",
    "print(f\"Model Intercept: {mlr2.intercept_}\")\n",
    "\n",
    "# generate predictions\n",
    "y2_pred = mlr2.predict(X2)\n",
    "\n",
    "# use predictions to score\n",
    "r2_small = r2_score(y2, y2_pred)\n",
    "mse_small = mean_squared_error(y2, y2_pred)\n",
    "rmse_small = np.sqrt(mse)\n",
    "\n",
    "print(f\"RÂ² Score: {r2_small:.4f}\")\n",
    "print(f\"RMSE: {rmse_small:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Very little change. Neither $R^2$ nor $RMSE$ improved. Despite lack of statistical significance, ML would likely keep all three predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
