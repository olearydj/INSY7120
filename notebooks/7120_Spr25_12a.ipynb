{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Classification Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook discusses the various model assessment metrics commonly used in Classification. It is based on Chapter 3 of HOML.\n",
    "\n",
    "We'll work with the MNIST dataset again, with a focus on the binary classifier that predicts \"5\" or \"not-5\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Taken from the 11b notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# use as_frame=False to get data as NumPy arrays\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Split into the predefined train and test sets\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# create boolean labels for the 5 / not-5 classifier\n",
    "y_train_5 = (y_train == '5')\n",
    "y_test_5 = (y_test == '5')\n",
    "\n",
    "# always scale features for LogReg\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# apply scaling without introducing data leakage\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logr_bin = LogisticRegression()\n",
    "\n",
    "logr_bin.fit(X_train_scaled, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Does it correctly predict an image we know is labeled \"5\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit = X[0]\n",
    "logr_bin.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "train_score = logr_bin.score(X_train_scaled, y_train_5)\n",
    "\n",
    "# Test accuracy (using the actual test set)\n",
    "test_score = logr_bin.score(X_test_scaled, y_test_5) \n",
    "# Note: You'll need to create y_test_5 with: y_test_5 = (y_test == '5')\n",
    "\n",
    "print(f\"Training accuracy: {train_score:.4f}\")\n",
    "print(f\"Test accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_test_pred = logr_bin.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_5, y_test_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_5, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "To better understand the consequence of FP vs FN errors, it is common to calculate performance metrics separately for each class. Except accuracy, which is inherently an overall measure of performance.\n",
    "\n",
    "The classification report is divided into by-class (top) and overall (bottom) scores.\n",
    "\n",
    "For the \"False\" class (negative class):\n",
    "- Precision: 0.98 (98% of predicted negatives were actually negative)\n",
    "- Recall: 0.99 (99% of actual negatives were correctly identified)\n",
    "- F1-score: 0.99 (harmonic mean of precision and recall)\n",
    "- Support: 9108 samples\n",
    "\n",
    "For the \"True\" class (positive class):\n",
    "- Precision: 0.90 (90% of predicted positives were actually positive)\n",
    "- Recall: 0.84 (84% of actual positives were correctly identified)\n",
    "- F1-score: 0.87 (harmonic mean of precision and recall)\n",
    "- Support: 892 samples\n",
    "\n",
    "Overall metrics:\n",
    "- Accuracy: 0.98 (98% of all predictions were correct)\n",
    "- Macro avg: 0.94 precision, 0.91 recall, 0.93 F1 (simple average across classes)\n",
    "- Weighted avg: 0.98 for precision, recall, and F1 (weighted by class support)\n",
    "\n",
    "Note: accuracy is an overall score unrelated to f1-score. The table is somewhat confusing in that regard. The macro and weighted average rows have values for precision, recall, and f1-score.\n",
    "\n",
    "Observations:\n",
    "- Imbalanced dataset (9108 negatives, 892 pos)\n",
    "- Performs better on majority \"False\" class\n",
    "- Good performance on the minority \"True\" class\n",
    "- The relatively high precision (0.90) for the \"True\" class indicates that when the model predicts \"True,\" it's usually correct, while the slightly lower recall (0.84) shows it misses some positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "For individual classifier metrics, use the appropriate SKL functions from `metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision_score(y_test_5, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test_5, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "From the recall, we see that when the model only detects 83.7% of the 5s in the test data.\n",
    "\n",
    "By default, these report the score for the positive class only, which match the results above. To score the negative class, use `pos_label=0` to trick it into compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test_5, y_test_pred, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test_5, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Precision / Recall Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "We've seen that precision and recall are influenced by the decision threshold. SKL does not let you set that value directly, but we can use the model's `decision_function` method (introduced in 11b), which returns a score for each observation. Then we can use any threshold to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = logr_bin.decision_function([some_digit])\n",
    "print(f\"Score for the first observation: {y_scores[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "(y_scores > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase threshold\n",
    "threshold = 150\n",
    "(y_scores > threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We know that `some_digit` is, in fact a 5. By default, our model correctly classifies it as such.\n",
    "\n",
    "For `LogisticRegression` a value of 0 produced by `decision_function` is equivalent to a 0.5 probability from `predict_proba`. So by setting the threshold to zero and comparing it with `y_scores = 132.7`, we are reproducing the normal classification.\n",
    "\n",
    "By repeating that comparison after increasing `threshold` to 150, we predict a not-5 result. This coifrms that raising the threshold decreases recall by increasing the number of FN results.\n",
    "\n",
    "So, we know that different decisions may benefit from higher recall or precision, and that this is controlled by the threshold. How do we decide what value to use for threshold?\n",
    "\n",
    "We can use `cross_val_predict` to get the scores of all instances in the training set, but tell it to return the z values instead of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_scores = cross_val_predict(logr_bin, X_train_scaled, y_train_5, cv=5, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Then use `precision_recall_curve` to compute both metrics for all possible thresholds, and use matplotlib to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "# beautify the figure: add grid, legend, axis, labels, and circles\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.xlabel(\"Threshold\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.title(\"Precision and Recall versus the decision threshold\")\n",
    "plt.xlim([-80, 20])\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "# Find the threshold value where precision and recall are approximately equal\n",
    "# (This appears to be around threshold = 0)\n",
    "crossing_point_threshold = thresholds[np.argmin(np.abs(precisions[:-1] - recalls[:-1]))]\n",
    "\n",
    "# Place circles at this point\n",
    "crossing_precision = precisions[np.argmin(np.abs(thresholds - crossing_point_threshold))]\n",
    "crossing_recall = recalls[np.argmin(np.abs(thresholds - crossing_point_threshold))]\n",
    "\n",
    "# Plot the circles\n",
    "plt.plot(crossing_point_threshold, crossing_precision, \"bo\", markersize=8)\n",
    "plt.plot(crossing_point_threshold, crossing_recall, \"go\", markersize=8)\n",
    "\n",
    "# Add the threshold line at this point\n",
    "plt.vlines(crossing_point_threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crossing_point_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot with proper sizing\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\", color='#1f77b4')\n",
    "\n",
    "# Beautify the figure\n",
    "plt.grid(True)\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision versus Recall', fontsize=14)\n",
    "plt.axis([0, 1.1, 0, 1.05])  # Set axis limits\n",
    "\n",
    "# Create custom legend with the point\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Based on this chart, how would you set your threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
