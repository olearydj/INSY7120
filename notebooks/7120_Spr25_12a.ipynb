{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Classification Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook discusses the various model assessment metrics commonly used in Classification. It is based on Chapter 3 of HOML.\n",
    "\n",
    "We'll work with the MNIST dataset again, with a focus on the binary classifier that predicts \"5\" or \"not-5\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Taken from the 11b notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# use as_frame=False to get data as NumPy arrays\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Split into the predefined train and test sets\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# create boolean labels for the 5 / not-5 classifier\n",
    "y_train_5 = (y_train == '5')\n",
    "y_test_5 = (y_test == '5')\n",
    "\n",
    "# always scale features for LogReg\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# apply scaling without introducing data leakage\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logr_bin = LogisticRegression()\n",
    "\n",
    "logr_bin.fit(X_train_scaled, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Does it correctly predict an image we know is labeled \"5\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG\n",
    "# some_digit = X[0]\n",
    "\n",
    "# must use scaled data as input for model predictions!\n",
    "some_digit = X_train_scaled[0]\n",
    "logr_bin.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "train_score = logr_bin.score(X_train_scaled, y_train_5)\n",
    "\n",
    "# Test accuracy (using the actual test set)\n",
    "test_score = logr_bin.score(X_test_scaled, y_test_5) \n",
    "# Note: You'll need to create y_test_5 with: y_test_5 = (y_test == '5')\n",
    "\n",
    "print(f\"Training accuracy: {train_score:.4f}\")\n",
    "print(f\"Test accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_test_pred = logr_bin.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_5, y_test_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_5, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "To better understand the consequence of FP vs FN errors, it is common to calculate performance metrics separately for each class. Except accuracy, which is inherently an overall measure of performance.\n",
    "\n",
    "The classification report is divided into by-class (top) and overall (bottom) scores.\n",
    "\n",
    "For the \"False\" class (negative class):\n",
    "- Precision: 0.98 (98% of predicted negatives were actually negative)\n",
    "- Recall: 0.99 (99% of actual negatives were correctly identified)\n",
    "- F1-score: 0.99 (harmonic mean of precision and recall)\n",
    "- Support: 9108 samples\n",
    "\n",
    "For the \"True\" class (positive class):\n",
    "- Precision: 0.90 (90% of predicted positives were actually positive)\n",
    "- Recall: 0.84 (84% of actual positives were correctly identified)\n",
    "- F1-score: 0.87 (harmonic mean of precision and recall)\n",
    "- Support: 892 samples\n",
    "\n",
    "Overall metrics:\n",
    "- Accuracy: 0.98 (98% of all predictions were correct)\n",
    "- Macro avg: 0.94 precision, 0.91 recall, 0.93 F1 (simple average across classes)\n",
    "- Weighted avg: 0.98 for precision, recall, and F1 (weighted by class support)\n",
    "\n",
    "Note: accuracy is an overall score unrelated to f1-score. The table is somewhat confusing in that regard. The macro and weighted average rows have values for precision, recall, and f1-score.\n",
    "\n",
    "Observations:\n",
    "- Imbalanced dataset (9108 negatives, 892 pos)\n",
    "- Performs better on majority \"False\" class\n",
    "- Good performance on the minority \"True\" class\n",
    "- The relatively high precision (0.90) for the \"True\" class indicates that when the model predicts \"True,\" it's usually correct, while the slightly lower recall (0.84) shows it misses some positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "For individual classifier metrics, use the appropriate SKL functions from `metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision_score(y_test_5, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test_5, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "From the recall, we see that when the model only detects 83.7% of the 5s in the test data.\n",
    "\n",
    "By default, these report the score for the positive class only, which match the results above. To score the negative class, use `pos_label=0` to trick it into compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test_5, y_test_pred, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test_5, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Precision / Recall Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "We've seen that precision and recall are influenced by the decision threshold. SKL does not let you set that value directly, but we can use the model's `decision_function` method (introduced in 11b), which returns a score for each observation. Then we can use any threshold to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = logr_bin.decision_function([some_digit])\n",
    "print(f\"Score for the first observation: {y_scores[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "(y_scores > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase threshold\n",
    "threshold = 2\n",
    "(y_scores > threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We know that `some_digit` is, in fact a 5. By default, our model correctly classifies it as such.\n",
    "\n",
    "For `LogisticRegression` a value of 0 produced by `decision_function` is equivalent to a 0.5 probability from `predict_proba`. So by setting the threshold to zero and comparing it with `y_scores = 1.9739`, we are reproducing the normal classification.\n",
    "\n",
    "By repeating that comparison after increasing `threshold` to 2, we predict a not-5 result. This coifrms that raising the threshold decreases recall by increasing the number of FN results.\n",
    "\n",
    "So, we know that different decisions may benefit from higher recall or precision, and that this is controlled by the threshold. How do we decide what value to use for threshold?\n",
    "\n",
    "We can use `cross_val_predict` to get the scores of all instances in the training set, but tell it to return the z values instead of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_scores = cross_val_predict(logr_bin, X_train_scaled, y_train_5, cv=5, method=\"decision_function\")\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Then use `precision_recall_curve` to compute both metrics for all possible thresholds, and use matplotlib to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))  # extra code – it's not needed, just formatting\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "plt.vlines(threshold, 0, 1.05, \"k\", \"dotted\", label=\"threshold\")\n",
    "\n",
    "# extra code – this section just beautifies and saves Figure 3–5\n",
    "idx = (thresholds >= threshold).argmax()  # first index ≥ threshold\n",
    "plt.plot(thresholds[idx], precisions[idx], \"bo\")\n",
    "plt.plot(thresholds[idx], recalls[idx], \"go\")\n",
    "plt.axis([-17, 17, 0, 1.05])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Threshold (decision_function)\")\n",
    "plt.legend(loc=\"center right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Note that the decision_function score for our `some_digit` example was still uncertain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_bin.predict_proba([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Another useful visualization is the Precision-Recall curve, which shows us the relationship between those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))  # extra code – not needed, just formatting\n",
    "\n",
    "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n",
    "\n",
    "# extra code – just beautifies and saves Figure 3–6\n",
    "plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n",
    "plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\n",
    "plt.plot([recalls[idx]], [precisions[idx]], \"ko\", label=\"Point above threshold 2.0\")\n",
    "plt.text(0.6, 0.62, \"Higher\\nthreshold\", color=\"#333333\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.axis([0, 1.0, 0, 1.05])\n",
    "plt.grid()\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "From this we can see that precision falls of quickly when recall exceeds about 0.8, corresponding to a precision of about 0.9.\n",
    "\n",
    "These two plots are complementary. Both can help us visualize the pr-tradeoff, but only the first can help us choose the corresponding threshold to use when generating new predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "We can also use the `argmax` method to find the corresponding threshold in the data, as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use argmax to find index of first threshold that gives at least 90% precision\n",
    "idx_90_prec = (precisions >= 0.90).argmax()\n",
    "\n",
    "# Get the corresponding threshold value\n",
    "thresh_90_prec = thresholds[idx_90_prec]\n",
    "\n",
    "print(f\"Threshold value: {thresh_90_prec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "This aligns well with the prior chart.\n",
    "\n",
    "To make predictions using that \"ideal\" threshold, simply create a boolean array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores >= thresh_90_prec)\n",
    "precision_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "These scores demonstrate that our method works, and that it is easy to generate predictions for any threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Calculate a point of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_for_threshold_at_90 = (thresholds <= thresh_90_prec).argmax()\n",
    "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Plot the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches  # for the curved arrow\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
    "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
    "\n",
    "plt.gca().add_patch(patches.FancyArrowPatch(\n",
    "    (0.20, 0.89), (0.07, 0.70),\n",
    "    connectionstyle=\"arc3,rad=.4\",\n",
    "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
    "    color=\"#444444\"))\n",
    "plt.text(0.12, 0.71, \"Higher\\nthreshold\", color=\"#333333\")\n",
    "plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.grid()\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Calculate area under the curve, ROC AUC score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---- Create a pipeline with preprocessing and model ----\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Standardize features\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))  # Default KNN model\n",
    "])\n",
    "\n",
    "# ---- Train the model ----\n",
    "pipeline.fit(X_train, y_train_5)\n",
    "\n",
    "# ---- Make predictions ----\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_prob = pipeline.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "# ---- Evaluate the model ----\n",
    "accuracy = accuracy_score(y_test_5, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_5, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_5, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = cross_val_predict(pipeline, X_train, y_train_5, cv=5, method=\"predict_proba\")[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_5, y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
    "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "#plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
    "\n",
    "plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.grid()\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
