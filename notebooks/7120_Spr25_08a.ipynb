{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Regression - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Feature selection approaches include both discrete and continuous methods. Discrete methods make binary in/out decisions for each feature. They use combinatorial optimization (testing discrete combinations) approaches that quickly become impractical or worse.\n",
    "\n",
    "Continuous methods apply a penalty to coefficients, reducing the importance of some features relative to another. They use continuous optimization (finding optimal values along a continuous penalty spectrum) approaches, which is much more computationally efficient.\n",
    "\n",
    "We will cover both, but the class will focus on regularization techniques. They provide a more nuanced way to control model complexity while avoiding the computational challenges and instability of subset selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Load the packages and configure environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Discrete Feature Selection - Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Discrete feature selection is not supported by SKL. We can use the `mlxtend` library by Sebastian Raschka to demonstrate these methods. It includes `ExhaustiveFeatureSelector`, which implements Best Subset, and `SequentialFeatureSelector`, which implements Forward and Backward Stepwise selection methods. Documentation for these can be found at [the mlxtend homepage](https://rasbt.github.io/mlxtend/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab, set up the environment\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Best Subset with `ExhaustiveFeatureSelector`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Using the Boston data from HW1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data set directly from the web using pandas\n",
    "url = \"https://raw.githubusercontent.com/olearydj/INSY7120/refs/heads/main/notebooks/data/Boston.csv\"\n",
    "boston = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Legend from HW1:\n",
    "\n",
    "- crim: per capita crime rate by town.\n",
    "- zn: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- indus: proportion of non-retail business acres per town.\n",
    "- chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- nox: nitrogen oxides concentration (parts per 10 million).\n",
    "- rm: average number of rooms per dwelling.\n",
    "- age: proportion of owner-occupied units built prior to 1940.\n",
    "- dis: weighted mean of distances to five Boston employment centres.\n",
    "- rad: index of accessibility to radial highways.\n",
    "- tax: full-value property-tax rate per \\$10,000.\n",
    "- ptratio: pupil-teacher ratio by town.\n",
    "- lstat: lower status of the population (percent).\n",
    "- medv: median value of owner-occupied homes in \\$1000s.\n",
    "\n",
    "There are 12 predictors and one outcome, `crim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictors of interest\n",
    "X = boston.loc[:,'zn':]\n",
    "y = boston[['crim']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Then use `ExhaustiveFeatureSelection` from mlxtend to specify the range of feature sets you want to explore.\n",
    "\n",
    "From [the documentation](https://arc.net/l/quote/xdfbqtco) (or `help(EFS)` after import), EFS takes a number of parameters:\n",
    "\n",
    "```text\n",
    "ExhaustiveFeatureSelector(estimator, min_features=1, max_features=1, print_progress=True, scoring='accuracy', cv=5, n_jobs=1, pre_dispatch='2*n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)\n",
    "```\n",
    "\n",
    "Of most immediate interest to us are:\n",
    "\n",
    "- `estimator`: the SKL classifier or regressor\n",
    "- `min_features`: the minimum number of features to select\n",
    "- `max_features`: the maximum number of features to select\n",
    "- `scoring`: the scoring metric used\n",
    "- `print_progress`: prints progress (default: True)\n",
    "\n",
    "The `scoring` parameter documentation here seems out of date. I believe that it can use any metric that is appropriate for regression. We have discussed RSE, MSE, RMSE, and R^2. To get a list of the metrics available in SKL: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer_names\n",
    "\n",
    "get_scorer_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Of these, `neg_mean_squared_error` (MSE), `neg_root_mean_squared_error` (RMSE), and `r2` match. RSE is not included. Note that all these values are negated (as indicated by the `neg_` prefix). This is to match SKL's convention that higher scoring values are better. Recall that low values of MSE and RMSE are good, so negating them ensures that higher values represent better models.\n",
    "\n",
    "To use EFS we create an instance of it with the desired parameters, including the model. Then we fit the resulting \"meta-model\" using `r2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "\n",
    "lr = LinearRegression()\n",
    "# turn off cross-validation and progress outputs\n",
    "efs = EFS(lr, min_features=5, max_features=12, scoring='r2', cv=0, print_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Testing all combinations of 5 to 12 features. This will take a sec. For $p$ predictors,\n",
    "\n",
    "$$\\text{models built} = \\sum_{k=min}^{max} \\binom{p}{k} = \\sum_{k=min}^{max} \\frac{p!}{k!(p-k)!}$$\n",
    "\n",
    "In our case, $p = 12$, $min = 5$, and $max = 12$, so the number of models is 3,302. Here is a simple function to calculate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_subset_models(total_predictors, min_features, max_features):\n",
    "    from math import comb\n",
    "    \n",
    "    # Ensure max_features doesn't exceed total predictors\n",
    "    max_features = min(max_features, total_predictors)\n",
    "    \n",
    "    # Sum up combinations for each feature count\n",
    "    total_models = 0\n",
    "    for k in range(min_features, max_features + 1):\n",
    "        total_models += comb(total_predictors, k)\n",
    "    \n",
    "    return total_models\n",
    "\n",
    "print(count_subset_models(12, 5, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "We can use the `%timeit` command in Jupyter Lab to time the execution. Normally this runs the specified command `r` times for each `n` loops so that mean and standard deviations can be reported. Here the `-r1` and `-n1` arguments tell it to perform the fit only once (one run, one loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r1 -n1 efs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The scores and subset are stored in the `best_score_` and `best_idx_` attributes of the fitted model. `best_feature_names_` holds the corresponding column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best R^2: {efs.best_score_:.2f}\")\n",
    "print(f\"Best Subset: {efs.best_idx_}\")\n",
    "print(efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Note: this process selected all the predictors. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "The `subsets_` attribute holds all the combinations tested and their scores. It is a dictionary with integer index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "efs.subsets_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "We can use its length to confirm that the expected number of models were fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(efs.subsets_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Now that we have the \"best\" features, we have to build and fit a new model, and evaluate the result.\n",
    "\n",
    "First, use the EFS method to transform the feature set. Then fit, etc. as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is equivalent to X_best = X[:, efs.best_idx_]\n",
    "X_best = efs.transform(X)\n",
    "\n",
    "lr_best = LinearRegression()\n",
    "lr_best.fit(X_best, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the estimated model parameters\n",
    "print(f\"Model Coefficients: {lr_best.coef_}\")\n",
    "print(f\"Model Intercept: {lr_best.intercept_}\")\n",
    "\n",
    "r2 = lr_best.score(X_best,y)\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Forward and Backward Stepwise with `SequentialFeatureSelector`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The `SequentialFeatureSelector` class implements both forward and backwards stepwise subset selection. It's interface (described at [this link](https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)) is similar to that used by EFS. There are differences in the parameters and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "lr = LinearRegression()\n",
    "# turn off cross-validation and progress outputs\n",
    "forward = SFS(lr, k_features=5, forward=True, scoring='r2', cv=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r1 -n1 forward.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2: {forward.k_score_:.2f}\")\n",
    "print(f\"Best Subset: {forward.k_feature_idx_}\")\n",
    "print(forward.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward.subsets_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Compare with backwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward = SFS(lr, k_features=5, forward=False, scoring='r2', cv=0)\n",
    "backward.fit(X, y)\n",
    "print(f\"R^2: {backward.k_score_:.2f}\")\n",
    "print(f\"Best Subset: {backward.k_feature_idx_}\")\n",
    "print(backward.k_feature_names_)\n",
    "backward.subsets_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Continuous Feature Selection - Regularization (aka Shrinkage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso are sensitive to predictors with different value scales. Need to standardize data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create a ridge regression model with regularization strength alpha=1.0\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit the model\n",
    "ridge_model.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Coefficients: {ridge_model.coef_}\")\n",
    "print(f\"Intercept: {ridge_model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R² score on training data\n",
    "r2 = ridge_model.score(X_scaled, y)\n",
    "print(f\"Ridge R² on training data: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### The Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create a ridge regression model with regularization strength alpha=1.0\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "\n",
    "# Fit the model\n",
    "lasso_model.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Coefficients: {lasso_model.coef_}\")\n",
    "print(f\"Intercept: {lasso_model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R² score on training data\n",
    "r2 = lasso_model.score(X_scaled, y)\n",
    "print(f\"Lasso R² on training data: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns\n",
    "non_zero_features = feature_names[lasso_model.coef_ != 0]\n",
    "print(non_zero_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Plots for Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define functions for variance and bias²\n",
    "def variance(lambda_val):\n",
    "    return 1 / (1 + lambda_val)\n",
    "\n",
    "def bias_squared(lambda_val):\n",
    "    return (lambda_val**2) / ((1 + lambda_val)**2)\n",
    "\n",
    "def mse(lambda_val):\n",
    "    # MSE = Variance + Bias² (ignoring irreducible error)\n",
    "    return variance(lambda_val) + bias_squared(lambda_val)\n",
    "\n",
    "# Create lambda values - focusing on 0 to 4 range\n",
    "lambda_vals = np.linspace(0, 4, 1000)\n",
    "\n",
    "# Calculate values\n",
    "var_vals = variance(lambda_vals)\n",
    "bias_sq_vals = bias_squared(lambda_vals)\n",
    "mse_vals = mse(lambda_vals)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the curves\n",
    "plt.plot(lambda_vals, var_vals, 'g-', linewidth=2.5, label='Variance ∝ 1/(1+λ)')\n",
    "plt.plot(lambda_vals, bias_sq_vals, 'k-', linewidth=2.5, label='Bias² ∝ λ²/(1+λ)²')\n",
    "plt.plot(lambda_vals, mse_vals, 'purple', linewidth=2.5, label='MSE = Variance + Bias²')\n",
    "\n",
    "# Find and mark the minimum MSE point\n",
    "min_mse_idx = np.argmin(mse_vals)\n",
    "min_mse_lambda = lambda_vals[min_mse_idx]\n",
    "min_mse_value = mse_vals[min_mse_idx]\n",
    "\n",
    "plt.plot(min_mse_lambda, min_mse_value, 'ro', markersize=8)\n",
    "plt.annotate(f'Optimal λ ≈ {min_mse_lambda:.2f}', \n",
    "             xy=(min_mse_lambda, min_mse_value),\n",
    "             xytext=(min_mse_lambda+0.5, min_mse_value+0.05),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=12)\n",
    "\n",
    "# Set up the plot\n",
    "plt.xlabel('λ (Lambda)', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.title('Variance-Bias² Tradeoff in Ridge Regression', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations to explain key regions - adjusted for zoomed view\n",
    "plt.annotate('Variance decreases rapidly', \n",
    "             xy=(0.3, variance(0.3)), \n",
    "             xytext=(0.5, 0.6),\n",
    "             arrowprops=dict(facecolor='green', shrink=0.05),\n",
    "             fontsize=12)\n",
    "\n",
    "plt.annotate('Bias² increases slowly at first', \n",
    "             xy=(0.5, bias_squared(0.5)), \n",
    "             xytext=(1.5, 0.1),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=12)\n",
    "\n",
    "plt.annotate('MSE improves', \n",
    "             xy=(0.5, mse(0.5)), \n",
    "             xytext=(1.0, 0.8),\n",
    "             arrowprops=dict(facecolor='purple', shrink=0.05),\n",
    "             fontsize=12)\n",
    "\n",
    "# Mark the OLS point (λ=0)\n",
    "plt.plot(0, variance(0), 'bo', markersize=8)\n",
    "plt.annotate('OLS (λ=0)', \n",
    "             xy=(0, variance(0)),\n",
    "             xytext=(0.2, 0.9),\n",
    "             arrowprops=dict(facecolor='blue', shrink=0.05),\n",
    "             fontsize=12)\n",
    "\n",
    "# Zoom in on the 0-4 lambda range as requested\n",
    "plt.xlim(-0.1, 4)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "\n",
    "# Add a clear marker at λ=0 showing variance=1, bias²=0\n",
    "plt.axhline(y=1.0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axhline(y=0.0, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For the second plot - rates of change\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Small delta for numerical differentiation\n",
    "delta = 0.0001\n",
    "lambda_vals_dense = np.linspace(0.0001, 4, 1000)  # Avoid division by zero\n",
    "\n",
    "# Compute numerical derivatives\n",
    "var_derivative = (variance(lambda_vals_dense + delta) - variance(lambda_vals_dense)) / delta\n",
    "bias_derivative = (bias_squared(lambda_vals_dense + delta) - bias_squared(lambda_vals_dense)) / delta\n",
    "\n",
    "# Plot the derivatives\n",
    "plt.plot(lambda_vals_dense, -var_derivative, 'g-', linewidth=2.5, \n",
    "         label='Rate of variance decrease')\n",
    "plt.plot(lambda_vals_dense, bias_derivative, 'k-', linewidth=2.5, \n",
    "         label='Rate of bias² increase')\n",
    "\n",
    "# Intersection point where rates are equal\n",
    "intersection_idx = np.argmin(np.abs(-var_derivative - bias_derivative))\n",
    "intersection_lambda = lambda_vals_dense[intersection_idx]\n",
    "\n",
    "plt.plot(intersection_lambda, -var_derivative[intersection_idx], 'ro', markersize=8)\n",
    "plt.annotate('Equal rates\\npoint', \n",
    "             xy=(intersection_lambda, -var_derivative[intersection_idx]),\n",
    "             xytext=(intersection_lambda+0.5, -var_derivative[intersection_idx]+0.1),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=12)\n",
    "\n",
    "# Set up the plot\n",
    "plt.xlabel('λ (Lambda)', fontsize=14)\n",
    "plt.ylabel('Rate of change', fontsize=14)\n",
    "plt.title('Rates of Change for Variance and Bias² with λ', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('Variance changes rapidly at small λ', \n",
    "             xy=(0.1, -var_derivative[10]), \n",
    "             xytext=(1.5, 0.8),\n",
    "             arrowprops=dict(facecolor='green', shrink=0.05),\n",
    "             fontsize=12)\n",
    "\n",
    "plt.annotate('Bias² changes slowly at small λ', \n",
    "             xy=(0.1, bias_derivative[10]), \n",
    "             xytext=(2, 0.2),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=12)\n",
    "\n",
    "# Zoom in on the 0-4 lambda range\n",
    "plt.xlim(-0.1, 4)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
